{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9738e86",
   "metadata": {},
   "source": [
    "# Phishing URL Linear Model Experiments\n",
    "\n",
    "This notebook explores various linear models using the Kaggle phishing URL dataset.\n",
    "\n",
    "In increasing order of complexity, we will experiment with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3af33e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             classification_report, roc_curve)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "037a68b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9143, 2)\n",
      "Test shape: (2286, 2)\n",
      "Train with features shape: (9143, 78)\n",
      "Test with features shape: (2286, 78)\n"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "train_w_features_df = pd.read_csv('dataset/df_train_feature_engineered.csv')\n",
    "test_w_features_df = pd.read_csv('dataset/df_test_feature_engineered.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "print(f\"Train with features shape: {train_w_features_df.shape}\")\n",
    "print(f\"Test with features shape: {test_w_features_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2d7557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'target', 'is_http', 'has_subdomain', 'has_tld', 'num_subdomain',\n",
       "       'is_domain_ip', 'num_hyphens_domain', 'is_punycode', 'has_path',\n",
       "       'path_depth', 'has_filename', 'has_file_extension', 'has_query',\n",
       "       'length_url', 'length_hostname', 'length_tld', 'length_sld',\n",
       "       'length_subdomains', 'length_path', 'length_query', 'num_dots',\n",
       "       'num_hyphens', 'num_at', 'num_question_marks', 'num_and', 'num_equal',\n",
       "       'num_percent', 'tld_in_path', 'tld_in_subdomain',\n",
       "       'subdomain_longer_sld', 'ratio_digits_url', 'ratio_digits_hostname',\n",
       "       'ratio_letter_url', 'ratio_path_url', 'ratio_hostname_url',\n",
       "       'length_words_url', 'avg_word_hostname', 'avg_word_path',\n",
       "       'num_unique_chars_hostname', 'has_shortened_hostname',\n",
       "       'entropy_hostname', 'has_www_subdomain', 'has_com_tld',\n",
       "       'is_http_and_many_subdomains', 'ip_and_short_tld',\n",
       "       'http_and_missing_domain_info', 'subdomain_depth_x_http', 'ip_x_http',\n",
       "       'domain_complexity_score', 'suspicion_score', 'contains_brand_misspell',\n",
       "       'is_homoglyph_attack', 'homoglyph_type', 'risk_score',\n",
       "       'is_zero_num_hyphens_domain', 'is_zero_length_subdomains',\n",
       "       'is_zero_num_hyphens', 'is_zero_num_at', 'is_zero_num_question_marks',\n",
       "       'is_zero_num_and', 'is_zero_num_equal', 'is_zero_num_percent',\n",
       "       'is_zero_ratio_digits_url', 'is_zero_ratio_digits_hostname',\n",
       "       'is_zero_avg_word_path', 'is_zero_length_query',\n",
       "       'num_subdomain_bucketed', 'length_tld_bucketed', 'path_depth_bucketed',\n",
       "       'log_length_url', 'log_length_path', 'log_ratio_hostname_url',\n",
       "       'log_length_words_url', 'log_avg_word_hostname',\n",
       "       'log_num_unique_chars_hostname', 'squared_ratio_letter_url',\n",
       "       'squared_entropy_hostname'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_features_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd5c46",
   "metadata": {},
   "source": [
    "Following the EDA, we use the transformed features and drop the original ones since linear models require normalized and scaled inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3287fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'target', 'is_http', 'has_subdomain', 'has_tld', 'is_domain_ip',\n",
       "       'is_punycode', 'has_path', 'has_filename', 'has_file_extension',\n",
       "       'has_query', 'length_hostname', 'length_sld', 'num_dots', 'tld_in_path',\n",
       "       'tld_in_subdomain', 'subdomain_longer_sld', 'ratio_path_url',\n",
       "       'has_shortened_hostname', 'has_www_subdomain', 'has_com_tld',\n",
       "       'is_http_and_many_subdomains', 'ip_and_short_tld',\n",
       "       'http_and_missing_domain_info', 'subdomain_depth_x_http', 'ip_x_http',\n",
       "       'domain_complexity_score', 'suspicion_score', 'contains_brand_misspell',\n",
       "       'is_homoglyph_attack', 'homoglyph_type', 'risk_score',\n",
       "       'is_zero_num_hyphens_domain', 'is_zero_length_subdomains',\n",
       "       'is_zero_num_hyphens', 'is_zero_num_at', 'is_zero_num_question_marks',\n",
       "       'is_zero_num_and', 'is_zero_num_equal', 'is_zero_num_percent',\n",
       "       'is_zero_ratio_digits_url', 'is_zero_ratio_digits_hostname',\n",
       "       'is_zero_avg_word_path', 'is_zero_length_query',\n",
       "       'num_subdomain_bucketed', 'length_tld_bucketed', 'path_depth_bucketed',\n",
       "       'log_length_url', 'log_length_path', 'log_ratio_hostname_url',\n",
       "       'log_length_words_url', 'log_avg_word_hostname',\n",
       "       'log_num_unique_chars_hostname', 'squared_ratio_letter_url',\n",
       "       'squared_entropy_hostname'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop original versions of log transformed features\n",
    "train_w_features_df.drop(columns=['length_url', 'length_path',  'ratio_hostname_url', 'length_words_url', 'avg_word_hostname', 'num_unique_chars_hostname'], inplace=True)\n",
    "\n",
    "# Drop original versions of squared transformed features\n",
    "train_w_features_df.drop(columns=['ratio_letter_url', 'entropy_hostname'], inplace=True)\n",
    "\n",
    "# Drop original versions of is_zero transformed features\n",
    "train_w_features_df.drop(columns=['num_hyphens_domain', 'length_subdomains', 'num_hyphens',  'num_at', 'num_question_marks', 'num_and', 'num_equal', 'num_percent', 'ratio_digits_url', 'ratio_digits_hostname', 'avg_word_path', 'length_query'], inplace=True)\n",
    "\n",
    "# Drop original versions of bucketed transformed features\n",
    "train_w_features_df.drop(columns=['num_subdomain', 'length_tld', 'path_depth'], inplace=True)\n",
    "\n",
    "# Check final columns\n",
    "train_w_features_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229250e",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "Now lets move on to training the models. We use the saver class to help us standardize the storing of metrics and models for evaluation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35654109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: darwin\n"
     ]
    }
   ],
   "source": [
    "# Import ModelSaver\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "from save_model import ModelSaver\n",
    "\n",
    "# Configuration\n",
    "SAVE_MODELS = True\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Check device (not strictly needed for sklearn but good for consistency)\n",
    "print(f\"Running on: {sys.platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f731fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 49 numeric/boolean features.\n",
      "Numeric Train Shape: (9143, 49)\n",
      "Numeric Test Shape: (2286, 49)\n",
      "Text Train Shape: (9143,)\n",
      "Text Test Shape: (2286,)\n",
      "Target Shape: (9143,)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Preparation ---\n",
    "\n",
    "# 1. Prepare Numeric Features\n",
    "# Select numeric and boolean columns and exclude target\n",
    "numeric_cols = train_w_features_df.select_dtypes(include=[np.number, bool]).columns.tolist()\n",
    "if 'target' in numeric_cols:\n",
    "    numeric_cols.remove('target')\n",
    "\n",
    "print(f\"Selected {len(numeric_cols)} numeric/boolean features.\")\n",
    "\n",
    "# Ensure boolean columns are converted to integers (0/1) for the model\n",
    "X_numeric = train_w_features_df[numeric_cols].astype(float).values\n",
    "y = train_w_features_df['target'].values\n",
    "\n",
    "# Prepare Test Data for Numeric\n",
    "X_numeric_test = test_w_features_df[numeric_cols].astype(float).values\n",
    "\n",
    "# 2. Prepare Text Features (URLs)\n",
    "X_text = train_df['url'].values\n",
    "X_text_test = test_df['url'].values\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Numeric Train Shape: {X_numeric.shape}\")\n",
    "print(f\"Numeric Test Shape: {X_numeric_test.shape}\")\n",
    "print(f\"Text Train Shape: {X_text.shape}\")\n",
    "print(f\"Text Test Shape: {X_text_test.shape}\")\n",
    "print(f\"Target Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74110c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba, threshold=0.5):\n",
    "    \"\"\"Calculate standard metrics for binary classification.\"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'TP': int(tp), 'FP': int(fp), 'TN': int(tn), 'FN': int(fn)\n",
    "    }\n",
    "\n",
    "def run_cv_experiment(X, y, X_test, pipeline_creator, experiment_name, model_name, vectorizer_name, params, feature_names_func=None):\n",
    "    \"\"\"\n",
    "    Run a cross-validation experiment and save results using ModelSaver.\n",
    "    \n",
    "    Args:\n",
    "        X: Training features\n",
    "        y: Training targets\n",
    "        X_test: Test features\n",
    "        pipeline_creator: Function that returns a fresh sklearn Pipeline\n",
    "        experiment_name: Name of the experiment for saving\n",
    "        model_name: Name of the model type\n",
    "        vectorizer_name: Name of the vectorizer/feature set\n",
    "        params: Dictionary containing 'model_params' and 'vectorizer_params'\n",
    "        feature_names_func: Optional function to extract feature names from fitted pipeline\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running Experiment: {experiment_name} ===\")\n",
    "    \n",
    "    saver = ModelSaver(base_path=\"experiments\")\n",
    "    saver.start_experiment(\n",
    "        experiment_name=experiment_name,\n",
    "        model_type=model_name,\n",
    "        vectorizer=vectorizer_name,\n",
    "        vectorizer_params=params.get('vectorizer_params', {}),\n",
    "        model_params=params.get('model_params', {}),\n",
    "        n_folds=N_FOLDS\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        print(f\"Fold {fold}/{N_FOLDS}\")\n",
    "        \n",
    "        # Split data\n",
    "        # Handle both numpy arrays and pandas series/dataframes if necessary, \n",
    "        # but we converted to numpy arrays in preparation step.\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and fit pipeline\n",
    "        pipeline = pipeline_creator()\n",
    "        pipeline.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Validation metrics\n",
    "        val_probs = pipeline.predict_proba(X_val_fold)[:, 1]\n",
    "        val_metrics = calculate_metrics(y_val_fold, val_probs)\n",
    "        val_metrics['fold'] = fold\n",
    "        \n",
    "        print(f\"  Val AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Test predictions (for ensemble later)\n",
    "        test_probs = pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Get feature names if possible\n",
    "        feature_names = None\n",
    "        if feature_names_func:\n",
    "            try:\n",
    "                feature_names = feature_names_func(pipeline)\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not extract feature names: {e}\")\n",
    "            \n",
    "        saver.add_fold(\n",
    "            fold_model=pipeline,\n",
    "            fold_metric=val_metrics,\n",
    "            test_predictions=test_probs,\n",
    "            feature_names=feature_names\n",
    "        )\n",
    "        \n",
    "    saver.finalize_experiment()\n",
    "    print(f\"Experiment saved to {saver._exp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d06d47",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression (Engineered Numeric Features)\n",
    "\n",
    "We first test a simple Logistic Regression model using only the manually engineered numeric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2c137f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Experiment: exp_1_numeric_lr ===\n",
      "Experiment 'exp_1_numeric_lr' initialized at: experiments/exp_1_numeric_lr\n",
      "Mode: Incremental saving (5 folds)\n",
      "Fold 1/5\n",
      "  Val AUC: 0.9450\n",
      "  Fold 1/5 saved | ROC AUC: 0.9450\n",
      "Fold 2/5\n",
      "  Val AUC: 0.9369\n",
      "  Fold 2/5 saved | ROC AUC: 0.9369\n",
      "Fold 3/5\n",
      "  Val AUC: 0.9431\n",
      "  Fold 3/5 saved | ROC AUC: 0.9431\n",
      "Fold 4/5\n",
      "  Val AUC: 0.9459\n",
      "  Fold 4/5 saved | ROC AUC: 0.9459\n",
      "Fold 5/5\n",
      "  Val AUC: 0.9404\n",
      "  Fold 5/5 saved | ROC AUC: 0.9404\n",
      "\n",
      "Finalizing experiment...\n",
      "  Predictions saved to experiments/exp_1_numeric_lr/exp_1_numeric_lr_prediction.csv\n",
      "\n",
      "✓ Experiment 'exp_1_numeric_lr' finalized!\n",
      "  Location: experiments/exp_1_numeric_lr\n",
      "  Folds completed: 5\n",
      "  Best fold: 4 (ROC AUC: 0.9459)\n",
      "  Average ROC AUC: 0.9423 ± 0.0033\n",
      "Experiment saved to experiments/exp_1_numeric_lr\n",
      "  Val AUC: 0.9404\n",
      "  Fold 5/5 saved | ROC AUC: 0.9404\n",
      "\n",
      "Finalizing experiment...\n",
      "  Predictions saved to experiments/exp_1_numeric_lr/exp_1_numeric_lr_prediction.csv\n",
      "\n",
      "✓ Experiment 'exp_1_numeric_lr' finalized!\n",
      "  Location: experiments/exp_1_numeric_lr\n",
      "  Folds completed: 5\n",
      "  Best fold: 4 (ROC AUC: 0.9459)\n",
      "  Average ROC AUC: 0.9423 ± 0.0033\n",
      "Experiment saved to experiments/exp_1_numeric_lr\n"
     ]
    }
   ],
   "source": [
    "def create_numeric_lr_pipeline():\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, solver='lbfgs'))\n",
    "    ])\n",
    "\n",
    "def get_numeric_feature_names(pipeline):\n",
    "    return numeric_cols\n",
    "\n",
    "numeric_params = {\n",
    "    'model_params': {'max_iter': 1000, 'solver': 'lbfgs'},\n",
    "    'vectorizer_params': {'type': 'StandardScaler'}\n",
    "}\n",
    "\n",
    "run_cv_experiment(\n",
    "    X=X_numeric, \n",
    "    y=y, \n",
    "    X_test=X_numeric_test,\n",
    "    pipeline_creator=create_numeric_lr_pipeline,\n",
    "    experiment_name=\"exp_1_numeric_lr\",\n",
    "    model_name=\"LogisticRegression\",\n",
    "    vectorizer_name=\"NumericFeatures\",\n",
    "    params=numeric_params,\n",
    "    feature_names_func=get_numeric_feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee0974",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression (TF-IDF Features)\n",
    "\n",
    "Next, we test Logistic Regression using TF-IDF features extracted directly from the URL strings. We use character n-grams to capture patterns in the URL structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15871e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Experiment: exp_1_tfidf_lr ===\n",
      "Experiment 'exp_1_tfidf_lr' initialized at: experiments/exp_1_tfidf_lr\n",
      "Mode: Incremental saving (5 folds)\n",
      "Fold 1/5\n",
      "  Val AUC: 0.9713\n",
      "  Fold 1/5 saved | ROC AUC: 0.9713\n",
      "Fold 2/5\n",
      "  Val AUC: 0.9713\n",
      "  Fold 1/5 saved | ROC AUC: 0.9713\n",
      "Fold 2/5\n",
      "  Val AUC: 0.9672\n",
      "  Fold 2/5 saved | ROC AUC: 0.9672\n",
      "Fold 3/5\n",
      "  Val AUC: 0.9672\n",
      "  Fold 2/5 saved | ROC AUC: 0.9672\n",
      "Fold 3/5\n",
      "  Val AUC: 0.9626\n",
      "  Fold 3/5 saved | ROC AUC: 0.9626\n",
      "Fold 4/5\n",
      "  Val AUC: 0.9626\n",
      "  Fold 3/5 saved | ROC AUC: 0.9626\n",
      "Fold 4/5\n",
      "  Val AUC: 0.9688\n",
      "  Fold 4/5 saved | ROC AUC: 0.9688\n",
      "Fold 5/5\n",
      "  Val AUC: 0.9688\n",
      "  Fold 4/5 saved | ROC AUC: 0.9688\n",
      "Fold 5/5\n",
      "  Val AUC: 0.9618\n",
      "  Fold 5/5 saved | ROC AUC: 0.9618\n",
      "\n",
      "Finalizing experiment...\n",
      "  Predictions saved to experiments/exp_1_tfidf_lr/exp_1_tfidf_lr_prediction.csv\n",
      "\n",
      "✓ Experiment 'exp_1_tfidf_lr' finalized!\n",
      "  Location: experiments/exp_1_tfidf_lr\n",
      "  Folds completed: 5\n",
      "  Best fold: 1 (ROC AUC: 0.9713)\n",
      "  Average ROC AUC: 0.9664 ± 0.0036\n",
      "Experiment saved to experiments/exp_1_tfidf_lr\n",
      "  Val AUC: 0.9618\n",
      "  Fold 5/5 saved | ROC AUC: 0.9618\n",
      "\n",
      "Finalizing experiment...\n",
      "  Predictions saved to experiments/exp_1_tfidf_lr/exp_1_tfidf_lr_prediction.csv\n",
      "\n",
      "✓ Experiment 'exp_1_tfidf_lr' finalized!\n",
      "  Location: experiments/exp_1_tfidf_lr\n",
      "  Folds completed: 5\n",
      "  Best fold: 1 (ROC AUC: 0.9713)\n",
      "  Average ROC AUC: 0.9664 ± 0.0036\n",
      "Experiment saved to experiments/exp_1_tfidf_lr\n"
     ]
    }
   ],
   "source": [
    "def create_tfidf_lr_pipeline():\n",
    "    return Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(3, 5))),\n",
    "        ('clf', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000))\n",
    "    ])\n",
    "\n",
    "def get_tfidf_feature_names(pipeline):\n",
    "    return pipeline.named_steps['tfidf'].get_feature_names_out().tolist()\n",
    "\n",
    "tfidf_params = {\n",
    "    'model_params': {'max_iter': 1000},\n",
    "    'vectorizer_params': {'max_features': 5000, 'analyzer': 'char', 'ngram_range': (3, 5)}\n",
    "}\n",
    "\n",
    "run_cv_experiment(\n",
    "    X=X_text, \n",
    "    y=y, \n",
    "    X_test=X_text_test,\n",
    "    pipeline_creator=create_tfidf_lr_pipeline,\n",
    "    experiment_name=\"exp_1_tfidf_lr\",\n",
    "    model_name=\"LogisticRegression\",\n",
    "    vectorizer_name=\"TfidfVectorizer\",\n",
    "    params=tfidf_params,\n",
    "    feature_names_func=get_tfidf_feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf17bc6",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression (Combined Features)\n",
    "\n",
    "Since we see that tf-idf features perform better, lets try combining both feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2ede24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Experiment: exp_1_combined_lr ===\n",
      "Experiment 'exp_1_combined_lr' initialized at: experiments/exp_1_combined_lr\n",
      "Mode: Incremental saving (5 folds)\n",
      "Fold 1/5\n",
      "  Val AUC: 0.9807\n",
      "  Fold 1/5 saved | ROC AUC: 0.9807\n",
      "Fold 2/5\n",
      "  Val AUC: 0.9807\n",
      "  Fold 1/5 saved | ROC AUC: 0.9807\n",
      "Fold 2/5\n",
      "  Val AUC: 0.9787\n",
      "  Fold 2/5 saved | ROC AUC: 0.9787\n",
      "Fold 3/5\n",
      "  Val AUC: 0.9787\n",
      "  Fold 2/5 saved | ROC AUC: 0.9787\n",
      "Fold 3/5\n",
      "  Val AUC: 0.9783\n",
      "  Fold 3/5 saved | ROC AUC: 0.9783\n",
      "Fold 4/5\n",
      "  Val AUC: 0.9783\n",
      "  Fold 3/5 saved | ROC AUC: 0.9783\n",
      "Fold 4/5\n",
      "  Val AUC: 0.9835\n",
      "  Fold 4/5 saved | ROC AUC: 0.9835\n",
      "Fold 5/5\n",
      "  Val AUC: 0.9835\n",
      "  Fold 4/5 saved | ROC AUC: 0.9835\n",
      "Fold 5/5\n",
      "  Val AUC: 0.9775\n",
      "  Fold 5/5 saved | ROC AUC: 0.9775\n",
      "\n",
      "Finalizing experiment...\n",
      "  Predictions saved to experiments/exp_1_combined_lr/exp_1_combined_lr_prediction.csv\n",
      "\n",
      "✓ Experiment 'exp_1_combined_lr' finalized!\n",
      "  Location: experiments/exp_1_combined_lr\n",
      "  Folds completed: 5\n",
      "  Best fold: 4 (ROC AUC: 0.9835)\n",
      "  Average ROC AUC: 0.9797 ± 0.0021\n",
      "Experiment saved to experiments/exp_1_combined_lr\n",
      "  Val AUC: 0.9775\n",
      "  Fold 5/5 saved | ROC AUC: 0.9775\n",
      "\n",
      "Finalizing experiment...\n",
      "  Predictions saved to experiments/exp_1_combined_lr/exp_1_combined_lr_prediction.csv\n",
      "\n",
      "✓ Experiment 'exp_1_combined_lr' finalized!\n",
      "  Location: experiments/exp_1_combined_lr\n",
      "  Folds completed: 5\n",
      "  Best fold: 4 (ROC AUC: 0.9835)\n",
      "  Average ROC AUC: 0.9797 ± 0.0021\n",
      "Experiment saved to experiments/exp_1_combined_lr\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Pre-compute combined features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(3, 5))\n",
    "X_text_tfidf = tfidf_vectorizer.fit_transform(X_text)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "X_combined = hstack([X_text_tfidf, X_numeric_scaled]).tocsr()\n",
    "X_combined_test = hstack([tfidf_vectorizer.transform(X_text_test), scaler.transform(X_numeric_test)]).tocsr()\n",
    "\n",
    "def create_combined_lr_pipeline():\n",
    "    return Pipeline([\n",
    "        ('clf', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000))\n",
    "    ])\n",
    "\n",
    "def get_combined_feature_names(pipeline):\n",
    "    tfidf_features = tfidf_vectorizer.get_feature_names_out().tolist()\n",
    "    return tfidf_features + numeric_cols\n",
    "\n",
    "combined_params = {\n",
    "    'model_params': {'max_iter': 1000},\n",
    "    'vectorizer_params': {'tfidf': {'max_features': 5000, 'analyzer': 'char', 'ngram_range': (3, 5)}, 'scaler': 'StandardScaler'}\n",
    "}\n",
    "\n",
    "run_cv_experiment(\n",
    "    X=X_combined, \n",
    "    y=y, \n",
    "    X_test=X_combined_test,\n",
    "    pipeline_creator=create_combined_lr_pipeline,\n",
    "    experiment_name=\"exp_1_combined_lr\",\n",
    "    model_name=\"LogisticRegression\",\n",
    "    vectorizer_name=\"CombinedFeatures\",\n",
    "    params=combined_params,\n",
    "    feature_names_func=get_combined_feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594fc18",
   "metadata": {},
   "source": [
    "### 4. SVM (Combined Features)\n",
    "\n",
    "Our combined features seem to perform better overall, telling us that both feature sets contribute useful information. Lets try using SVM to see if accuracy improves further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feb8d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Experiment: exp_1_combined_svm ===\n",
      "Experiment 'exp_1_combined_svm' initialized at: experiments/exp_1_combined_svm\n",
      "Mode: Incremental saving (5 folds)\n",
      "Fold 1/5\n",
      "  Val AUC: 0.9845\n",
      "  Val AUC: 0.9845\n",
      "  Fold 1/5 saved | ROC AUC: 0.9845\n",
      "Fold 2/5\n",
      "  Fold 1/5 saved | ROC AUC: 0.9845\n",
      "Fold 2/5\n",
      "  Val AUC: 0.9831\n",
      "  Val AUC: 0.9831\n",
      "  Fold 2/5 saved | ROC AUC: 0.9831\n",
      "Fold 3/5\n",
      "  Fold 2/5 saved | ROC AUC: 0.9831\n",
      "Fold 3/5\n",
      "  Val AUC: 0.9829\n",
      "  Val AUC: 0.9829\n",
      "  Fold 3/5 saved | ROC AUC: 0.9829\n",
      "Fold 4/5\n",
      "  Fold 3/5 saved | ROC AUC: 0.9829\n",
      "Fold 4/5\n",
      "  Val AUC: 0.9865\n",
      "  Val AUC: 0.9865\n",
      "  Fold 4/5 saved | ROC AUC: 0.9865\n",
      "Fold 5/5\n",
      "  Fold 4/5 saved | ROC AUC: 0.9865\n",
      "Fold 5/5\n",
      "  Val AUC: 0.9823\n",
      "  Val AUC: 0.9823\n",
      "  Fold 5/5 saved | ROC AUC: 0.9823\n",
      "\n",
      "Finalizing experiment...\n",
      "  Predictions saved to experiments/exp_1_combined_svm/exp_1_combined_svm_prediction.csv\n",
      "\n",
      "✓ Experiment 'exp_1_combined_svm' finalized!\n",
      "  Location: experiments/exp_1_combined_svm\n",
      "  Folds completed: 5\n",
      "  Best fold: 4 (ROC AUC: 0.9865)\n",
      "  Average ROC AUC: 0.9838 ± 0.0015\n",
      "Experiment saved to experiments/exp_1_combined_svm\n",
      "  Fold 5/5 saved | ROC AUC: 0.9823\n",
      "\n",
      "Finalizing experiment...\n",
      "  Predictions saved to experiments/exp_1_combined_svm/exp_1_combined_svm_prediction.csv\n",
      "\n",
      "✓ Experiment 'exp_1_combined_svm' finalized!\n",
      "  Location: experiments/exp_1_combined_svm\n",
      "  Folds completed: 5\n",
      "  Best fold: 4 (ROC AUC: 0.9865)\n",
      "  Average ROC AUC: 0.9838 ± 0.0015\n",
      "Experiment saved to experiments/exp_1_combined_svm\n"
     ]
    }
   ],
   "source": [
    "def create_combined_svm_pipeline():\n",
    "    return Pipeline([\n",
    "        ('clf', SVC(kernel='linear', C=1.0, random_state=RANDOM_STATE, probability=True))\n",
    "    ])\n",
    "\n",
    "combined_svm_params = {\n",
    "    'model_params': {'kernel': 'linear', 'C': 1.0, 'probability': True},\n",
    "    'vectorizer_params': {'tfidf': {'max_features': 5000, 'analyzer': 'char', 'ngram_range': (3, 5)}, 'scaler': 'StandardScaler'}\n",
    "}\n",
    "\n",
    "run_cv_experiment(\n",
    "    X=X_combined, \n",
    "    y=y, \n",
    "    X_test=X_combined_test,\n",
    "    pipeline_creator=create_combined_svm_pipeline,\n",
    "    experiment_name=\"exp_1_combined_svm\",\n",
    "    model_name=\"SVM\",\n",
    "    vectorizer_name=\"CombinedFeatures\",\n",
    "    params=combined_svm_params,\n",
    "    feature_names_func=get_combined_feature_names\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
