{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f10526b",
   "metadata": {},
   "source": [
    "# Phishing URL Neural Network Experiments\n",
    "\n",
    "This notebook explores various neural network architectures using the Kaggle phishing URL dataset.\n",
    "\n",
    "In increasing order of complexity, we will experiment with:\n",
    "\n",
    "1. Simple Feedforward Neural Networks (MLP)\n",
    "2. Convolutional Neural Networks (CNN)\n",
    "3. Recurrent Neural Networks (RNN)\n",
    "4. Hybrid Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d20de9b",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43c0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             classification_report, roc_curve)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9143, 2)\n",
      "Test shape: (2286, 2)\n",
      "Train with features shape: (9143, 78)\n",
      "Test with features shape: (2286, 78)\n"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "train_w_features_df = pd.read_csv('dataset/df_train_feature_engineered.csv')\n",
    "test_w_features_df = pd.read_csv('dataset/df_test_feature_engineered.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "print(f\"Train with features shape: {train_w_features_df.shape}\")\n",
    "print(f\"Test with features shape: {test_w_features_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f0776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'target', 'is_http', 'has_subdomain', 'has_tld', 'num_subdomain',\n",
       "       'is_domain_ip', 'num_hyphens_domain', 'is_punycode', 'has_path',\n",
       "       'path_depth', 'has_filename', 'has_file_extension', 'has_query',\n",
       "       'length_url', 'length_hostname', 'length_tld', 'length_sld',\n",
       "       'length_subdomains', 'length_path', 'length_query', 'num_dots',\n",
       "       'num_hyphens', 'num_at', 'num_question_marks', 'num_and', 'num_equal',\n",
       "       'num_percent', 'tld_in_path', 'tld_in_subdomain',\n",
       "       'subdomain_longer_sld', 'ratio_digits_url', 'ratio_digits_hostname',\n",
       "       'ratio_letter_url', 'ratio_path_url', 'ratio_hostname_url',\n",
       "       'length_words_url', 'avg_word_hostname', 'avg_word_path',\n",
       "       'num_unique_chars_hostname', 'has_shortened_hostname',\n",
       "       'entropy_hostname', 'has_www_subdomain', 'has_com_tld',\n",
       "       'is_http_and_many_subdomains', 'ip_and_short_tld',\n",
       "       'http_and_missing_domain_info', 'subdomain_depth_x_http', 'ip_x_http',\n",
       "       'domain_complexity_score', 'suspicion_score', 'contains_brand_misspell',\n",
       "       'is_homoglyph_attack', 'homoglyph_type', 'risk_score',\n",
       "       'is_zero_num_hyphens_domain', 'is_zero_length_subdomains',\n",
       "       'is_zero_num_hyphens', 'is_zero_num_at', 'is_zero_num_question_marks',\n",
       "       'is_zero_num_and', 'is_zero_num_equal', 'is_zero_num_percent',\n",
       "       'is_zero_ratio_digits_url', 'is_zero_ratio_digits_hostname',\n",
       "       'is_zero_avg_word_path', 'is_zero_length_query',\n",
       "       'num_subdomain_bucketed', 'length_tld_bucketed', 'path_depth_bucketed',\n",
       "       'log_length_url', 'log_length_path', 'log_ratio_hostname_url',\n",
       "       'log_length_words_url', 'log_avg_word_hostname',\n",
       "       'log_num_unique_chars_hostname', 'squared_ratio_letter_url',\n",
       "       'squared_entropy_hostname'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_features_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9b87a",
   "metadata": {},
   "source": [
    "Following the EDA, we use the same feature set as log regression since MLP models require normalized and scaled inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b5067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'target', 'is_http', 'has_subdomain', 'has_tld', 'is_domain_ip',\n",
       "       'is_punycode', 'has_path', 'has_filename', 'has_file_extension',\n",
       "       'has_query', 'length_hostname', 'length_sld', 'num_dots', 'tld_in_path',\n",
       "       'tld_in_subdomain', 'subdomain_longer_sld', 'ratio_path_url',\n",
       "       'has_shortened_hostname', 'has_www_subdomain', 'has_com_tld',\n",
       "       'is_http_and_many_subdomains', 'ip_and_short_tld',\n",
       "       'http_and_missing_domain_info', 'subdomain_depth_x_http', 'ip_x_http',\n",
       "       'domain_complexity_score', 'suspicion_score', 'contains_brand_misspell',\n",
       "       'is_homoglyph_attack', 'homoglyph_type', 'risk_score',\n",
       "       'is_zero_num_hyphens_domain', 'is_zero_length_subdomains',\n",
       "       'is_zero_num_hyphens', 'is_zero_num_at', 'is_zero_num_question_marks',\n",
       "       'is_zero_num_and', 'is_zero_num_equal', 'is_zero_num_percent',\n",
       "       'is_zero_ratio_digits_url', 'is_zero_ratio_digits_hostname',\n",
       "       'is_zero_avg_word_path', 'is_zero_length_query',\n",
       "       'num_subdomain_bucketed', 'length_tld_bucketed', 'path_depth_bucketed',\n",
       "       'log_length_url', 'log_length_path', 'log_ratio_hostname_url',\n",
       "       'log_length_words_url', 'log_avg_word_hostname',\n",
       "       'log_num_unique_chars_hostname', 'squared_ratio_letter_url',\n",
       "       'squared_entropy_hostname'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop original versions of log transformed features\n",
    "train_w_features_df.drop(columns=['length_url', 'length_path',  'ratio_hostname_url', 'length_words_url', 'avg_word_hostname', 'num_unique_chars_hostname'], inplace=True)\n",
    "\n",
    "# Drop original versions of squared transformed features\n",
    "train_w_features_df.drop(columns=['ratio_letter_url', 'entropy_hostname'], inplace=True)\n",
    "\n",
    "# Drop original versions of is_zero transformed features\n",
    "train_w_features_df.drop(columns=['num_hyphens_domain', 'length_subdomains', 'num_hyphens',  'num_at', 'num_question_marks', 'num_and', 'num_equal', 'num_percent', 'ratio_digits_url', 'ratio_digits_hostname', 'avg_word_path', 'length_query'], inplace=True)\n",
    "\n",
    "# Drop original versions of bucketed transformed features\n",
    "train_w_features_df.drop(columns=['num_subdomain', 'length_tld', 'path_depth'], inplace=True)\n",
    "\n",
    "# Check final columns\n",
    "train_w_features_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecde92f",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "Now lets move on to training the models. We use the saver class to help us standardize the storing of metrics and models for evaluation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2796d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAVE_MODELS = True  # Switch to turn on/off model saving\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_URL_LEN = 200  # Truncate/pad URLs to this length\n",
    "EMBEDDING_DIM = 64\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Import ModelSaver\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "from save_model import ModelSaver\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preprocessing for Neural Networks ---\n",
    "\n",
    "# 0. URL Visualizer (New)\n",
    "class URLVisualizer:\n",
    "    def __init__(self, font_size=14, image_size=(200, 32)):\n",
    "        self.font_size = font_size\n",
    "        self.image_size = image_size\n",
    "        try:\n",
    "            self.font = ImageFont.load_default()\n",
    "        except:\n",
    "            self.font = None\n",
    "            \n",
    "    def transform(self, urls):\n",
    "        images = []\n",
    "        for url in urls:\n",
    "            img = Image.new('L', self.image_size, color=255)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            draw.text((5, 5), str(url), font=self.font, fill=0)\n",
    "            img_np = np.array(img, dtype=np.float32) / 255.0\n",
    "            images.append(img_np)\n",
    "        return np.array(images)[:, np.newaxis, :, :]\n",
    "\n",
    "# Initialize Visualizer\n",
    "visualizer = URLVisualizer()\n",
    "print(\"Rendering URLs to images... (this may take a moment)\")\n",
    "X_images_train = visualizer.transform(train_df['url'])\n",
    "X_images_test = visualizer.transform(test_df['url'])\n",
    "print(f\"Images shape: {X_images_train.shape}\")\n",
    "\n",
    "# 1. Character Tokenizer for URLs\n",
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        self.char2idx = {}\n",
    "        self.idx2char = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        unique_chars = set()\n",
    "        for text in texts:\n",
    "            unique_chars.update(str(text))\n",
    "        \n",
    "        self.char2idx = {char: idx + 2 for idx, char in enumerate(sorted(unique_chars))}\n",
    "        self.char2idx['<PAD>'] = 0\n",
    "        self.char2idx['<UNK>'] = 1\n",
    "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
    "        self.vocab_size = len(self.char2idx)\n",
    "        \n",
    "    def transform(self, texts, max_len):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            text = str(text)\n",
    "            seq = [self.char2idx.get(c, 1) for c in text]\n",
    "            if len(seq) < max_len:\n",
    "                seq = seq + [0] * (max_len - len(seq))\n",
    "            else:\n",
    "                seq = seq[:max_len]\n",
    "            sequences.append(seq)\n",
    "        return np.array(sequences)\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "tokenizer.fit(train_df['url'])\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# 2. Prepare Numeric Features\n",
    "numeric_cols = train_w_features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'target' in numeric_cols:\n",
    "    numeric_cols.remove('target')\n",
    "\n",
    "X_numeric_train = train_w_features_df[numeric_cols].values\n",
    "X_numeric_test = test_w_features_df[numeric_cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numeric_train_scaled = scaler.fit_transform(X_numeric_train)\n",
    "X_numeric_test_scaled = scaler.transform(X_numeric_test)\n",
    "\n",
    "# 3. Prepare Text Features\n",
    "X_text_train = tokenizer.transform(train_df['url'], MAX_URL_LEN)\n",
    "X_text_test = tokenizer.transform(test_df['url'], MAX_URL_LEN)\n",
    "\n",
    "# 4. Prepare Targets\n",
    "y_train = train_df['target'].values\n",
    "if 'target' in test_df.columns:\n",
    "    y_test = test_df['target'].values\n",
    "else:\n",
    "    y_test = np.zeros(len(test_df))\n",
    "\n",
    "# 5. Create PyTorch Datasets\n",
    "class PhishingDataset(Dataset):\n",
    "    def __init__(self, X_numeric, X_text, y, X_images=None):\n",
    "        self.X_numeric = torch.FloatTensor(X_numeric)\n",
    "        self.X_text = torch.LongTensor(X_text)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.X_images = torch.FloatTensor(X_images) if X_images is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.X_images is not None:\n",
    "            return self.X_numeric[idx], self.X_text[idx], self.X_images[idx], self.y[idx]\n",
    "        return self.X_numeric[idx], self.X_text[idx], self.y[idx]\n",
    "\n",
    "full_train_dataset = PhishingDataset(X_numeric_train_scaled, X_text_train, y_train, X_images_train)\n",
    "test_dataset = PhishingDataset(X_numeric_test_scaled, X_text_test, y_test, X_images_test)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"Data preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training & Evaluation Functions ---\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        x_img = None\n",
    "        if len(batch) == 3:\n",
    "            x_num, x_txt, y = batch\n",
    "        else:\n",
    "            x_num, x_txt, x_img, y = batch\n",
    "            x_img = x_img.to(device)\n",
    "            \n",
    "        x_num, x_txt, y = x_num.to(device), x_txt.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_num, x_txt, x_img).squeeze()\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * x_num.size(0)\n",
    "        all_preds.extend(outputs.detach().cpu().numpy())\n",
    "        all_targets.extend(y.detach().cpu().numpy())\n",
    "        \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x_img = None\n",
    "            if len(batch) == 3:\n",
    "                x_num, x_txt, y = batch\n",
    "            else:\n",
    "                x_num, x_txt, x_img, y = batch\n",
    "                x_img = x_img.to(device)\n",
    "                \n",
    "            x_num, x_txt, y = x_num.to(device), x_txt.to(device), y.to(device)\n",
    "            \n",
    "            outputs = model(x_num, x_txt, x_img).squeeze()\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            running_loss += loss.item() * x_num.size(0)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "            \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy_score(all_targets, (all_preds > 0.5).astype(int)),\n",
    "        'precision': precision_score(all_targets, (all_preds > 0.5).astype(int), zero_division=0),\n",
    "        'recall': recall_score(all_targets, (all_preds > 0.5).astype(int), zero_division=0),\n",
    "        'f1': f1_score(all_targets, (all_preds > 0.5).astype(int), zero_division=0),\n",
    "        'roc_auc': roc_auc_score(all_targets, all_preds)\n",
    "    }\n",
    "    \n",
    "    return metrics, all_preds\n",
    "\n",
    "def run_experiment(model_class, model_name, model_params, experiment_name, save_model=True):\n",
    "    print(f\"\\n=== Running Experiment: {experiment_name} ({model_name}) ===\")\n",
    "    print(f\"Saving Model: {save_model}\")\n",
    "    \n",
    "    saver = None\n",
    "    if save_model:\n",
    "        saver = ModelSaver(base_path=\"experiments\")\n",
    "        saver.start_experiment(\n",
    "            experiment_name=experiment_name,\n",
    "            model_type=model_name,\n",
    "            vectorizer=\"CharTokenizer/Visualizer\",\n",
    "            vectorizer_params={'vocab_size': tokenizer.vocab_size, 'max_len': MAX_URL_LEN},\n",
    "            model_params=model_params,\n",
    "            n_folds=5,\n",
    "            save_format=\"pickle\"\n",
    "        )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_test_preds = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_numeric_train_scaled, y_train), start=1):\n",
    "        print(f\"\\n--- Fold {fold}/5 ---\")\n",
    "        \n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(full_train_dataset, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "        val_loader = DataLoader(full_train_dataset, batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "        \n",
    "        model = model_class(**model_params).to(device)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        best_val_auc = 0.0\n",
    "        best_model_state = None\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_metrics, _ = evaluate(model, val_loader, criterion, device)\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f} - Val AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "                \n",
    "            if val_metrics['roc_auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['roc_auc']\n",
    "                best_model_state = model.state_dict()\n",
    "                \n",
    "        if best_model_state:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            \n",
    "        val_metrics, val_preds = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Fold {fold} Best Val AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        test_metrics, test_preds = evaluate(model, test_loader, criterion, device)\n",
    "        fold_test_preds.append(test_preds)\n",
    "        \n",
    "        if save_model and saver:\n",
    "            saver.add_fold(\n",
    "                fold_model=model,\n",
    "                fold_metric={\"fold\": fold, **val_metrics, \"train_size\": len(train_idx), \"val_size\": len(val_idx)},\n",
    "                test_predictions=test_preds,\n",
    "                feature_names=[\"numeric_features\"]\n",
    "            )\n",
    "            \n",
    "    if save_model and saver:\n",
    "        saver.finalize_experiment()\n",
    "        print(f\"Experiment saved to {saver._exp_dir}\")\n",
    "        \n",
    "    return model, val_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff2055",
   "metadata": {},
   "source": [
    "### 1. Baseline MLP (Numeric Features Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa032f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Baseline MLP Architecture\n",
    "class BaselineMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.3):\n",
    "        super(BaselineMLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x_numeric, x_text=None, x_img=None):\n",
    "        # Ignores x_text, x_img\n",
    "        return torch.sigmoid(self.network(x_numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4672c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Baseline MLP Experiment\n",
    "SAVE_MLP = True # Set to False to skip saving\n",
    "\n",
    "mlp_params = {\n",
    "    'input_dim': X_numeric_train_scaled.shape[1],\n",
    "    'hidden_dims': [128, 64, 32],\n",
    "    'dropout': DROPOUT\n",
    "}\n",
    "\n",
    "run_experiment(BaselineMLP, \"BaselineMLP\", mlp_params, \"exp_3_mlp_baseline\", save_model=SAVE_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956638e5",
   "metadata": {},
   "source": [
    "### 2. CharCNN (Text Features Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759973fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CharCNN Architecture\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, num_filters=128, filter_sizes=[3, 4, 5], dropout=0.3, max_len=200):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, \n",
    "                      out_channels=num_filters, \n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, 1)\n",
    "        \n",
    "    def forward(self, x_numeric, x_text, x_img=None):\n",
    "        # Ignores x_numeric, x_img\n",
    "        # x_text shape: [batch_size, max_len]\n",
    "        embedded = self.embedding(x_text) # [batch_size, max_len, emb_dim]\n",
    "        \n",
    "        # Permute for Conv1d: [batch_size, emb_dim, max_len]\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply Convs + ReLU + MaxPool\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # Concatenate\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        dropped = self.dropout(cat)\n",
    "        \n",
    "        return torch.sigmoid(self.fc(dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CharCNN Experiment\n",
    "SAVE_CNN = True # Set to False to skip saving\n",
    "\n",
    "cnn_params = {\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'num_filters': 128,\n",
    "    'filter_sizes': [3, 4, 5],\n",
    "    'dropout': DROPOUT,\n",
    "    'max_len': MAX_URL_LEN\n",
    "}\n",
    "\n",
    "run_experiment(CharCNN, \"CharCNN\", cnn_params, \"exp_3_charcnn\", save_model=SAVE_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835b310",
   "metadata": {},
   "source": [
    "### 3. Visual CNN (Rendered URLs)\n",
    "\n",
    "Inspired by DeepSeek OCR and other vision-based text approaches, we treat the URL as an image.\n",
    "We render the URL text onto a black-and-white image and process it with a 2D CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bad140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Visual CNN Architecture\n",
    "class VisualCNN(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 32, 200), num_filters=32, dropout=0.3):\n",
    "        super(VisualCNN, self).__init__()\n",
    "        # Input: [batch, 1, 32, 200]\n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) # -> [16, 100]\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters*2, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2) # -> [8, 50]\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(num_filters*2, num_filters*4, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2) # -> [4, 25]\n",
    "        \n",
    "        self.flatten_dim = num_filters * 4 * 4 * 25\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_numeric, x_text, x_img):\n",
    "        # Ignores x_numeric, x_text\n",
    "        x = F.relu(self.conv1(x_img))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# Run Visual CNN Experiment\n",
    "SAVE_VISUAL = True\n",
    "\n",
    "visual_params = {\n",
    "    'input_shape': (1, 32, 200),\n",
    "    'num_filters': 32,\n",
    "    'dropout': DROPOUT\n",
    "}\n",
    "\n",
    "run_experiment(VisualCNN, \"VisualCNN\", visual_params, \"exp_3_visual_cnn\", save_model=SAVE_VISUAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77540748",
   "metadata": {},
   "source": [
    "### 4. BiLSTM (Text Features Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BiLSTM Architecture\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=num_layers, \n",
    "                            bidirectional=True, \n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Bidirectional = 2 * hidden_dim\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "    def forward(self, x_numeric, x_text, x_img=None):\n",
    "        # Ignores x_numeric, x_img\n",
    "        embedded = self.embedding(x_text)\n",
    "        \n",
    "        # LSTM output: output, (hidden, cell)\n",
    "        # We use the final hidden state or max pooling. \n",
    "        # Here we'll use the final hidden state of the last layer\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Concat the final forward and backward hidden states\n",
    "        # hidden shape: [num_layers * num_directions, batch, hidden_dim]\n",
    "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        dropped = self.dropout(hidden_cat)\n",
    "        return torch.sigmoid(self.fc(dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6735069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BiLSTM Experiment\n",
    "SAVE_LSTM = True # Set to False to skip saving\n",
    "\n",
    "lstm_params = {\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'hidden_dim': 128,\n",
    "    'num_layers': 2,\n",
    "    'dropout': DROPOUT\n",
    "}\n",
    "\n",
    "run_experiment(BiLSTM, \"BiLSTM\", lstm_params, \"exp_3_bilstm\", save_model=SAVE_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad21eb",
   "metadata": {},
   "source": [
    "### 5. Hybrid Model (Numeric + Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hybrid Model Architecture\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, vocab_size, numeric_input_dim, embedding_dim=64, num_filters=128, filter_sizes=[3, 4, 5], dropout=0.3):\n",
    "        super(HybridModel, self).__init__()\n",
    "        \n",
    "        # Text Branch (CNN)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, \n",
    "                      out_channels=num_filters, \n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.text_out_dim = len(filter_sizes) * num_filters\n",
    "        \n",
    "        # Numeric Branch (MLP)\n",
    "        self.numeric_fc = nn.Sequential(\n",
    "            nn.Linear(numeric_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.numeric_out_dim = 64\n",
    "        \n",
    "        # Combined\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.text_out_dim + self.numeric_out_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_numeric, x_text, x_img=None):\n",
    "        # Text Path\n",
    "        embedded = self.embedding(x_text).permute(0, 2, 1)\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        text_features = torch.cat(pooled, dim=1)\n",
    "        \n",
    "        # Numeric Path\n",
    "        numeric_features = self.numeric_fc(x_numeric)\n",
    "        \n",
    "        # Combine\n",
    "        combined = torch.cat((text_features, numeric_features), dim=1)\n",
    "        return torch.sigmoid(self.fc_final(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Hybrid Model Experiment\n",
    "SAVE_HYBRID = True # Set to False to skip saving\n",
    "\n",
    "hybrid_params = {\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'numeric_input_dim': X_numeric_train_scaled.shape[1],\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'num_filters': 128,\n",
    "    'filter_sizes': [3, 4, 5],\n",
    "    'dropout': DROPOUT\n",
    "}\n",
    "\n",
    "run_experiment(HybridModel, \"HybridModel\", hybrid_params, \"exp_3_hybrid\", save_model=SAVE_HYBRID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
