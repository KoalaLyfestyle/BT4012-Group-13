{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f10526b",
      "metadata": {
        "id": "6f10526b"
      },
      "source": [
        "# Phishing URL Neural Network Experiments\n",
        "\n",
        "This notebook explores various neural network architectures using the Kaggle phishing URL dataset.\n",
        "\n",
        "In increasing order of complexity, we will experiment with:\n",
        "\n",
        "1. Simple Feedforward Neural Networks (MLP)\n",
        "2. Convolutional Neural Networks (CNN)\n",
        "3. Recurrent Neural Networks (RNN)\n",
        "4. Hybrid Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d20de9b",
      "metadata": {
        "id": "1d20de9b"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee38e06d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "use_drive = False\n",
        "\n",
        "# uncomment if running on colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "use_drive = True\n",
        "drive_root = '/content/drive/MyDrive/fraud-grp-proj/'\n",
        "\n",
        "# check path exists\n",
        "import os\n",
        "print(os.path.exists(drive_root))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323dc638",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix,\n",
        "                             classification_report, roc_curve)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7a4c3d5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a4c3d5a",
        "outputId": "6750c583-bcbf-4dd0-b131-8968d00fb384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (9143, 2)\n",
            "Test shape: (2286, 2)\n",
            "Train with features shape: (9143, 78)\n",
            "Test with features shape: (2286, 78)\n"
          ]
        }
      ],
      "source": [
        "# Load train and test datasets\n",
        "train_df = pd.read_csv('dataset/train.csv')\n",
        "test_df = pd.read_csv('dataset/test.csv')\n",
        "\n",
        "train_w_features_df = pd.read_csv('dataset/df_train_feature_engineered.csv')\n",
        "test_w_features_df = pd.read_csv('dataset/df_test_feature_engineered.csv')\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "\n",
        "print(f\"Train with features shape: {train_w_features_df.shape}\")\n",
        "print(f\"Test with features shape: {test_w_features_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "663f0776",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "663f0776",
        "outputId": "ff08044b-8ffa-4e04-b0dd-466f908259aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['url', 'target', 'is_http', 'has_subdomain', 'has_tld', 'num_subdomain',\n",
              "       'is_domain_ip', 'num_hyphens_domain', 'is_punycode', 'has_path',\n",
              "       'path_depth', 'has_filename', 'has_file_extension', 'has_query',\n",
              "       'length_url', 'length_hostname', 'length_tld', 'length_sld',\n",
              "       'length_subdomains', 'length_path', 'length_query', 'num_dots',\n",
              "       'num_hyphens', 'num_at', 'num_question_marks', 'num_and', 'num_equal',\n",
              "       'num_percent', 'tld_in_path', 'tld_in_subdomain',\n",
              "       'subdomain_longer_sld', 'ratio_digits_url', 'ratio_digits_hostname',\n",
              "       'ratio_letter_url', 'ratio_path_url', 'ratio_hostname_url',\n",
              "       'length_words_url', 'avg_word_hostname', 'avg_word_path',\n",
              "       'num_unique_chars_hostname', 'has_shortened_hostname',\n",
              "       'entropy_hostname', 'has_www_subdomain', 'has_com_tld',\n",
              "       'is_http_and_many_subdomains', 'ip_and_short_tld',\n",
              "       'http_and_missing_domain_info', 'subdomain_depth_x_http', 'ip_x_http',\n",
              "       'domain_complexity_score', 'suspicion_score', 'contains_brand_misspell',\n",
              "       'is_homoglyph_attack', 'homoglyph_type', 'risk_score',\n",
              "       'is_zero_num_hyphens_domain', 'is_zero_length_subdomains',\n",
              "       'is_zero_num_hyphens', 'is_zero_num_at', 'is_zero_num_question_marks',\n",
              "       'is_zero_num_and', 'is_zero_num_equal', 'is_zero_num_percent',\n",
              "       'is_zero_ratio_digits_url', 'is_zero_ratio_digits_hostname',\n",
              "       'is_zero_avg_word_path', 'is_zero_length_query',\n",
              "       'num_subdomain_bucketed', 'length_tld_bucketed', 'path_depth_bucketed',\n",
              "       'log_length_url', 'log_length_path', 'log_ratio_hostname_url',\n",
              "       'log_length_words_url', 'log_avg_word_hostname',\n",
              "       'log_num_unique_chars_hostname', 'squared_ratio_letter_url',\n",
              "       'squared_entropy_hostname'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_w_features_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cf9b87a",
      "metadata": {
        "id": "8cf9b87a"
      },
      "source": [
        "Following the EDA, we use the same feature set as log regression since MLP models require normalized and scaled inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "215b5067",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "215b5067",
        "outputId": "76993343-0f77-4104-ece2-641483ff9c27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['url', 'target', 'is_http', 'has_subdomain', 'has_tld', 'is_domain_ip',\n",
              "       'is_punycode', 'has_path', 'has_filename', 'has_file_extension',\n",
              "       'has_query', 'length_hostname', 'length_sld', 'num_dots', 'tld_in_path',\n",
              "       'tld_in_subdomain', 'subdomain_longer_sld', 'ratio_path_url',\n",
              "       'has_shortened_hostname', 'has_www_subdomain', 'has_com_tld',\n",
              "       'is_http_and_many_subdomains', 'ip_and_short_tld',\n",
              "       'http_and_missing_domain_info', 'subdomain_depth_x_http', 'ip_x_http',\n",
              "       'domain_complexity_score', 'suspicion_score', 'contains_brand_misspell',\n",
              "       'is_homoglyph_attack', 'homoglyph_type', 'risk_score',\n",
              "       'is_zero_num_hyphens_domain', 'is_zero_length_subdomains',\n",
              "       'is_zero_num_hyphens', 'is_zero_num_at', 'is_zero_num_question_marks',\n",
              "       'is_zero_num_and', 'is_zero_num_equal', 'is_zero_num_percent',\n",
              "       'is_zero_ratio_digits_url', 'is_zero_ratio_digits_hostname',\n",
              "       'is_zero_avg_word_path', 'is_zero_length_query',\n",
              "       'num_subdomain_bucketed', 'length_tld_bucketed', 'path_depth_bucketed',\n",
              "       'log_length_url', 'log_length_path', 'log_ratio_hostname_url',\n",
              "       'log_length_words_url', 'log_avg_word_hostname',\n",
              "       'log_num_unique_chars_hostname', 'squared_ratio_letter_url',\n",
              "       'squared_entropy_hostname'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Drop original versions of log transformed features\n",
        "train_w_features_df.drop(columns=['length_url', 'length_path',  'ratio_hostname_url', 'length_words_url', 'avg_word_hostname', 'num_unique_chars_hostname'], inplace=True)\n",
        "\n",
        "# Drop original versions of squared transformed features\n",
        "train_w_features_df.drop(columns=['ratio_letter_url', 'entropy_hostname'], inplace=True)\n",
        "\n",
        "# Drop original versions of is_zero transformed features\n",
        "train_w_features_df.drop(columns=['num_hyphens_domain', 'length_subdomains', 'num_hyphens',  'num_at', 'num_question_marks', 'num_and', 'num_equal', 'num_percent', 'ratio_digits_url', 'ratio_digits_hostname', 'avg_word_path', 'length_query'], inplace=True)\n",
        "\n",
        "# Drop original versions of bucketed transformed features\n",
        "train_w_features_df.drop(columns=['num_subdomain', 'length_tld', 'path_depth'], inplace=True)\n",
        "\n",
        "# Check final columns\n",
        "train_w_features_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ecde92f",
      "metadata": {
        "id": "0ecde92f"
      },
      "source": [
        "## Training Models\n",
        "\n",
        "Now lets move on to training the models. We use the `ModelSaver` utility to help us standardize the storing of metrics and models for evaluation later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91abfe5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "SAVE_MODELS = True  # Switch to turn on/off model saving\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "MAX_URL_LEN = 200  # Truncate/pad URLs to this length\n",
        "EMBEDDING_DIM = 64\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# Import ModelSaver\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('.'))\n",
        "from save_model import ModelSaver\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "63bfe2ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63bfe2ae",
        "outputId": "2001980a-2522-4f4a-c131-df588ebbb6f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 101\n",
            "Data preparation complete.\n"
          ]
        }
      ],
      "source": [
        "# --- Data Preprocessing for Neural Networks ---\n",
        "\n",
        "# 1. Character Tokenizer for URLs\n",
        "class CharTokenizer:\n",
        "    def __init__(self):\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def fit(self, texts):\n",
        "        unique_chars = set()\n",
        "        for text in texts:\n",
        "            unique_chars.update(str(text))\n",
        "\n",
        "        self.char2idx = {char: idx + 2 for idx, char in enumerate(sorted(unique_chars))}\n",
        "        self.char2idx['<PAD>'] = 0\n",
        "        self.char2idx['<UNK>'] = 1\n",
        "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
        "        self.vocab_size = len(self.char2idx)\n",
        "\n",
        "    def transform(self, texts, max_len):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            text = str(text)\n",
        "            seq = [self.char2idx.get(c, 1) for c in text]\n",
        "            if len(seq) < max_len:\n",
        "                seq = seq + [0] * (max_len - len(seq))\n",
        "            else:\n",
        "                seq = seq[:max_len]\n",
        "            sequences.append(seq)\n",
        "        return np.array(sequences)\n",
        "\n",
        "tokenizer = CharTokenizer()\n",
        "tokenizer.fit(train_df['url'])\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# 2. Prepare Numeric Features\n",
        "numeric_cols = train_w_features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'target' in numeric_cols:\n",
        "    numeric_cols.remove('target')\n",
        "\n",
        "X_numeric_train = train_w_features_df[numeric_cols].values\n",
        "X_numeric_test = test_w_features_df[numeric_cols].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_numeric_train_scaled = scaler.fit_transform(X_numeric_train)\n",
        "X_numeric_test_scaled = scaler.transform(X_numeric_test)\n",
        "\n",
        "# 3. Prepare Text Features\n",
        "X_text_train = tokenizer.transform(train_df['url'], MAX_URL_LEN)\n",
        "X_text_test = tokenizer.transform(test_df['url'], MAX_URL_LEN)\n",
        "\n",
        "# 4. Prepare Targets\n",
        "y_train = train_df['target'].values\n",
        "if 'target' in test_df.columns:\n",
        "    y_test = test_df['target'].values\n",
        "else:\n",
        "    y_test = np.zeros(len(test_df))\n",
        "\n",
        "# 5. Create PyTorch Datasets\n",
        "class PhishingDataset(Dataset):\n",
        "    def __init__(self, X_numeric, X_text, y, X_images=None):\n",
        "        self.X_numeric = torch.FloatTensor(X_numeric)\n",
        "        self.X_text = torch.LongTensor(X_text)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "        self.X_images = torch.FloatTensor(X_images) if X_images is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.X_images is not None:\n",
        "            return self.X_numeric[idx], self.X_text[idx], self.X_images[idx], self.y[idx]\n",
        "        return self.X_numeric[idx], self.X_text[idx], self.y[idx]\n",
        "\n",
        "full_train_dataset = PhishingDataset(X_numeric_train_scaled, X_text_train, y_train)\n",
        "test_dataset = PhishingDataset(X_numeric_test_scaled, X_text_test, y_test)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "print(\"Data preparation complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d095085a",
      "metadata": {
        "id": "d095085a"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch in loader:\n",
        "        x_img = None\n",
        "        if len(batch) == 3:\n",
        "            x_num, x_txt, y = batch\n",
        "        else:\n",
        "            x_num, x_txt, x_img, y = batch\n",
        "            x_img = x_img.to(device)\n",
        "\n",
        "        x_num, x_txt, y = x_num.to(device), x_txt.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_num, x_txt, x_img).squeeze()\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x_num.size(0)\n",
        "        all_preds.extend(outputs.detach().cpu().numpy())\n",
        "        all_targets.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            x_img = None\n",
        "            if len(batch) == 3:\n",
        "                x_num, x_txt, y = batch\n",
        "            else:\n",
        "                x_num, x_txt, x_img, y = batch\n",
        "                x_img = x_img.to(device)\n",
        "\n",
        "            x_num, x_txt, y = x_num.to(device), x_txt.to(device), y.to(device)\n",
        "\n",
        "            outputs = model(x_num, x_txt, x_img).squeeze()\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            running_loss += loss.item() * x_num.size(0)\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    binary_preds = (all_preds > 0.5).astype(int)\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    tn, fp, fn, tp = confusion_matrix(all_targets, binary_preds).ravel()\n",
        "\n",
        "    metrics = {\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': accuracy_score(all_targets, binary_preds),\n",
        "        'precision': precision_score(all_targets, binary_preds, zero_division=0),\n",
        "        'recall': recall_score(all_targets, binary_preds, zero_division=0),\n",
        "        'f1': f1_score(all_targets, binary_preds, zero_division=0),\n",
        "        'roc_auc': roc_auc_score(all_targets, all_preds),\n",
        "        'TP': tp,\n",
        "        'FP': fp,\n",
        "        'TN': tn,\n",
        "        'FN': fn\n",
        "    }\n",
        "\n",
        "    return metrics, all_preds\n",
        "\n",
        "def run_experiment(model_class, model_name, model_params, experiment_name, save_model=True):\n",
        "    print(f\"\\n=== Running Experiment: {experiment_name} ({model_name}) ===\")\n",
        "    print(f\"Saving Model: {save_model}\")\n",
        "\n",
        "    saver = None\n",
        "    if save_model:\n",
        "        if use_drive:\n",
        "            base_path = drive_root + \"experiments\"\n",
        "        else:\n",
        "            base_path = \"experiments\"\n",
        "        saver = ModelSaver(base_path=base_path)\n",
        "        saver.start_experiment(\n",
        "            experiment_name=experiment_name,\n",
        "            model_type=model_name,\n",
        "            vectorizer=\"CharTokenizer/Visualizer\",\n",
        "            vectorizer_params={'vocab_size': tokenizer.vocab_size, 'max_len': MAX_URL_LEN},\n",
        "            model_params=model_params,\n",
        "            n_folds=5,\n",
        "            save_format=\"pickle\"\n",
        "        )\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_test_preds = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_numeric_train_scaled, y_train), start=1):\n",
        "        print(f\"\\n--- Fold {fold}/5 ---\")\n",
        "\n",
        "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "        train_loader = DataLoader(full_train_dataset, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
        "        val_loader = DataLoader(full_train_dataset, batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
        "\n",
        "        model = model_class(**model_params).to(device)\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        best_val_auc = 0.0\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            val_metrics, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f} - Val AUC: {val_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "            if val_metrics['roc_auc'] > best_val_auc:\n",
        "                best_val_auc = val_metrics['roc_auc']\n",
        "                best_model_state = model.state_dict()\n",
        "\n",
        "        if best_model_state:\n",
        "            model.load_state_dict(best_model_state)\n",
        "\n",
        "        val_metrics, val_preds = evaluate(model, val_loader, criterion, device)\n",
        "        print(f\"Fold {fold} Best Val AUC: {val_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "        test_metrics, test_preds = evaluate(model, test_loader, criterion, device)\n",
        "        fold_test_preds.append(test_preds)\n",
        "\n",
        "        if save_model and saver:\n",
        "            saver.add_fold(\n",
        "                fold_model=model,\n",
        "                fold_metric={\"fold\": fold, **val_metrics, \"train_size\": len(train_idx), \"val_size\": len(val_idx)},\n",
        "                test_predictions=test_preds,\n",
        "                feature_names=[\"numeric_features\"]\n",
        "            )\n",
        "\n",
        "    if save_model and saver:\n",
        "        saver.finalize_experiment()\n",
        "        print(f\"Experiment saved to {saver._exp_dir}\")\n",
        "\n",
        "    return model, val_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ff2055",
      "metadata": {
        "id": "c8ff2055"
      },
      "source": [
        "### 1. Baseline MLP (Numeric Features Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cfa032f2",
      "metadata": {
        "id": "cfa032f2"
      },
      "outputs": [],
      "source": [
        "# Define Baseline MLP Architecture\n",
        "class BaselineMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.3):\n",
        "        super(BaselineMLP, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev_dim = dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x_numeric, x_text=None, x_img=None):\n",
        "        # Ignores x_text, x_img\n",
        "        return torch.sigmoid(self.network(x_numeric))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "eb4672c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb4672c7",
        "outputId": "7fa7e96e-37af-43e8-9271-c10603729a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Running Experiment: exp_3_mlp_baseline (BaselineMLP) ===\n",
            "Saving Model: True\n",
            "Experiment 'exp_3_mlp_baseline' initialized at: /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_mlp_baseline\n",
            "Mode: Incremental saving (5 folds)\n",
            "\n",
            "--- Fold 1/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.2794 - Val AUC: 0.9429\n",
            "Epoch 10/20 - Train Loss: 0.2553 - Val AUC: 0.9509\n",
            "Epoch 15/20 - Train Loss: 0.2392 - Val AUC: 0.9541\n",
            "Epoch 20/20 - Train Loss: 0.2319 - Val AUC: 0.9564\n",
            "Fold 1 Best Val AUC: 0.9564\n",
            "  Fold 1/5 saved | ROC AUC: 0.9564\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.2755 - Val AUC: 0.9318\n",
            "Epoch 10/20 - Train Loss: 0.2555 - Val AUC: 0.9417\n",
            "Epoch 15/20 - Train Loss: 0.2385 - Val AUC: 0.9454\n",
            "Epoch 20/20 - Train Loss: 0.2302 - Val AUC: 0.9479\n",
            "Fold 2 Best Val AUC: 0.9479\n",
            "  Fold 2/5 saved | ROC AUC: 0.9479\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.2794 - Val AUC: 0.9403\n",
            "Epoch 10/20 - Train Loss: 0.2527 - Val AUC: 0.9473\n",
            "Epoch 15/20 - Train Loss: 0.2386 - Val AUC: 0.9516\n",
            "Epoch 20/20 - Train Loss: 0.2281 - Val AUC: 0.9546\n",
            "Fold 3 Best Val AUC: 0.9546\n",
            "  Fold 3/5 saved | ROC AUC: 0.9546\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.2817 - Val AUC: 0.9392\n",
            "Epoch 10/20 - Train Loss: 0.2584 - Val AUC: 0.9467\n",
            "Epoch 15/20 - Train Loss: 0.2397 - Val AUC: 0.9519\n",
            "Epoch 20/20 - Train Loss: 0.2267 - Val AUC: 0.9545\n",
            "Fold 4 Best Val AUC: 0.9545\n",
            "  Fold 4/5 saved | ROC AUC: 0.9545\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.2779 - Val AUC: 0.9379\n",
            "Epoch 10/20 - Train Loss: 0.2514 - Val AUC: 0.9449\n",
            "Epoch 15/20 - Train Loss: 0.2369 - Val AUC: 0.9507\n",
            "Epoch 20/20 - Train Loss: 0.2303 - Val AUC: 0.9525\n",
            "Fold 5 Best Val AUC: 0.9525\n",
            "  Fold 5/5 saved | ROC AUC: 0.9525\n",
            "\n",
            "Finalizing experiment...\n",
            "  Predictions saved to /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_mlp_baseline/exp_3_mlp_baseline_prediction.csv\n",
            "\n",
            "✓ Experiment 'exp_3_mlp_baseline' finalized!\n",
            "  Location: /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_mlp_baseline\n",
            "  Folds completed: 5\n",
            "  Best fold: 1 (ROC AUC: 0.9564)\n",
            "  Average ROC AUC: 0.9532 ± 0.0029\n",
            "Experiment saved to /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_mlp_baseline\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(BaselineMLP(\n",
              "   (network): Sequential(\n",
              "     (0): Linear(in_features=16, out_features=128, bias=True)\n",
              "     (1): ReLU()\n",
              "     (2): Dropout(p=0.3, inplace=False)\n",
              "     (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Dropout(p=0.3, inplace=False)\n",
              "     (6): Linear(in_features=64, out_features=32, bias=True)\n",
              "     (7): ReLU()\n",
              "     (8): Dropout(p=0.3, inplace=False)\n",
              "     (9): Linear(in_features=32, out_features=1, bias=True)\n",
              "   )\n",
              " ),\n",
              " {'loss': 0.0554269311022057,\n",
              "  'accuracy': 0.8862144420131292,\n",
              "  'precision': 0.9002267573696145,\n",
              "  'recall': 0.8687089715536105,\n",
              "  'f1': 0.8841870824053452,\n",
              "  'roc_auc': np.float64(0.9525003710814992),\n",
              "  'TP': np.int64(794),\n",
              "  'FP': np.int64(88),\n",
              "  'TN': np.int64(826),\n",
              "  'FN': np.int64(120)},\n",
              " {'loss': 0.2800612984810184,\n",
              "  'accuracy': 0.8858267716535433,\n",
              "  'precision': 0.8972972972972973,\n",
              "  'recall': 0.8713910761154856,\n",
              "  'f1': 0.8841544607190412,\n",
              "  'roc_auc': np.float64(0.9518262863686222),\n",
              "  'TP': np.int64(996),\n",
              "  'FP': np.int64(114),\n",
              "  'TN': np.int64(1029),\n",
              "  'FN': np.int64(147)})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run Baseline MLP Experiment\n",
        "SAVE_MLP = True # Set to False to skip saving\n",
        "\n",
        "mlp_params = {\n",
        "    'input_dim': X_numeric_train_scaled.shape[1],\n",
        "    'hidden_dims': [128, 64, 32],\n",
        "    'dropout': DROPOUT\n",
        "}\n",
        "\n",
        "run_experiment(BaselineMLP, \"BaselineMLP\", mlp_params, \"exp_3_mlp_baseline\", save_model=SAVE_MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956638e5",
      "metadata": {
        "id": "956638e5"
      },
      "source": [
        "### 2. CharCNN (Text Features Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "759973fa",
      "metadata": {
        "id": "759973fa"
      },
      "outputs": [],
      "source": [
        "# Define CharCNN Architecture\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=64, num_filters=128, filter_sizes=[3, 4, 5], dropout=0.3, max_len=200):\n",
        "        super(CharCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_filters,\n",
        "                      kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, 1)\n",
        "\n",
        "    def forward(self, x_numeric, x_text, x_img=None):\n",
        "        # Ignores x_numeric, x_img\n",
        "        # x_text shape: [batch_size, max_len]\n",
        "        embedded = self.embedding(x_text) # [batch_size, max_len, emb_dim]\n",
        "\n",
        "        # Permute for Conv1d: [batch_size, emb_dim, max_len]\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        # Apply Convs + ReLU + MaxPool\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        # Concatenate\n",
        "        cat = torch.cat(pooled, dim=1)\n",
        "        dropped = self.dropout(cat)\n",
        "\n",
        "        return torch.sigmoid(self.fc(dropped))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "120d6504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "120d6504",
        "outputId": "81202907-8422-425a-eacc-e96c3da87aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Running Experiment: exp_3_charcnn (CharCNN) ===\n",
            "Saving Model: True\n",
            "Experiment 'exp_3_charcnn' initialized at: /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_charcnn\n",
            "Mode: Incremental saving (5 folds)\n",
            "\n",
            "--- Fold 1/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1562 - Val AUC: 0.9780\n",
            "Epoch 10/20 - Train Loss: 0.0796 - Val AUC: 0.9818\n",
            "Epoch 15/20 - Train Loss: 0.0476 - Val AUC: 0.9830\n",
            "Epoch 20/20 - Train Loss: 0.0313 - Val AUC: 0.9833\n",
            "Fold 1 Best Val AUC: 0.9833\n",
            "  Fold 1/5 saved | ROC AUC: 0.9833\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1501 - Val AUC: 0.9695\n",
            "Epoch 10/20 - Train Loss: 0.0843 - Val AUC: 0.9750\n",
            "Epoch 15/20 - Train Loss: 0.0496 - Val AUC: 0.9770\n",
            "Epoch 20/20 - Train Loss: 0.0323 - Val AUC: 0.9767\n",
            "Fold 2 Best Val AUC: 0.9767\n",
            "  Fold 2/5 saved | ROC AUC: 0.9767\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1583 - Val AUC: 0.9685\n",
            "Epoch 10/20 - Train Loss: 0.0874 - Val AUC: 0.9761\n",
            "Epoch 15/20 - Train Loss: 0.0506 - Val AUC: 0.9767\n",
            "Epoch 20/20 - Train Loss: 0.0288 - Val AUC: 0.9779\n",
            "Fold 3 Best Val AUC: 0.9779\n",
            "  Fold 3/5 saved | ROC AUC: 0.9779\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1517 - Val AUC: 0.9749\n",
            "Epoch 10/20 - Train Loss: 0.0828 - Val AUC: 0.9794\n",
            "Epoch 15/20 - Train Loss: 0.0476 - Val AUC: 0.9804\n",
            "Epoch 20/20 - Train Loss: 0.0296 - Val AUC: 0.9808\n",
            "Fold 4 Best Val AUC: 0.9808\n",
            "  Fold 4/5 saved | ROC AUC: 0.9808\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1617 - Val AUC: 0.9709\n",
            "Epoch 10/20 - Train Loss: 0.0919 - Val AUC: 0.9765\n",
            "Epoch 15/20 - Train Loss: 0.0541 - Val AUC: 0.9794\n",
            "Epoch 20/20 - Train Loss: 0.0353 - Val AUC: 0.9803\n",
            "Fold 5 Best Val AUC: 0.9803\n",
            "  Fold 5/5 saved | ROC AUC: 0.9803\n",
            "\n",
            "Finalizing experiment...\n",
            "  Predictions saved to /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_charcnn/exp_3_charcnn_prediction.csv\n",
            "\n",
            "✓ Experiment 'exp_3_charcnn' finalized!\n",
            "  Location: /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_charcnn\n",
            "  Folds completed: 5\n",
            "  Best fold: 1 (ROC AUC: 0.9833)\n",
            "  Average ROC AUC: 0.9798 ± 0.0023\n",
            "Experiment saved to /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_charcnn\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(CharCNN(\n",
              "   (embedding): Embedding(101, 64, padding_idx=0)\n",
              "   (convs): ModuleList(\n",
              "     (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
              "     (1): Conv1d(64, 128, kernel_size=(4,), stride=(1,))\n",
              "     (2): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
              "   )\n",
              "   (dropout): Dropout(p=0.3, inplace=False)\n",
              "   (fc): Linear(in_features=384, out_features=1, bias=True)\n",
              " ),\n",
              " {'loss': 0.03929209689610012,\n",
              "  'accuracy': 0.9321663019693655,\n",
              "  'precision': 0.9312227074235808,\n",
              "  'recall': 0.9332603938730853,\n",
              "  'f1': 0.9322404371584699,\n",
              "  'roc_auc': np.float64(0.9803171190668857),\n",
              "  'TP': np.int64(853),\n",
              "  'FP': np.int64(63),\n",
              "  'TN': np.int64(851),\n",
              "  'FN': np.int64(61)},\n",
              " {'loss': 0.19567685759416925,\n",
              "  'accuracy': 0.9278215223097113,\n",
              "  'precision': 0.9237435008665511,\n",
              "  'recall': 0.9326334208223972,\n",
              "  'f1': 0.9281671745755333,\n",
              "  'roc_auc': np.float64(0.9811481351357765),\n",
              "  'TP': np.int64(1066),\n",
              "  'FP': np.int64(88),\n",
              "  'TN': np.int64(1055),\n",
              "  'FN': np.int64(77)})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run CharCNN Experiment\n",
        "SAVE_CNN = True # Set to False to skip saving\n",
        "\n",
        "cnn_params = {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'num_filters': 128,\n",
        "    'filter_sizes': [3, 4, 5],\n",
        "    'dropout': DROPOUT,\n",
        "    'max_len': MAX_URL_LEN\n",
        "}\n",
        "\n",
        "run_experiment(CharCNN, \"CharCNN\", cnn_params, \"exp_3_charcnn\", save_model=SAVE_CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77540748",
      "metadata": {
        "id": "77540748"
      },
      "source": [
        "### 3. BiLSTM (Text Features Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1b7bee73",
      "metadata": {
        "id": "1b7bee73"
      },
      "outputs": [],
      "source": [
        "# Define BiLSTM Architecture\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2, dropout=0.3):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Bidirectional = 2 * hidden_dim\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, x_numeric, x_text, x_img=None):\n",
        "        # Ignores x_numeric, x_img\n",
        "        embedded = self.embedding(x_text)\n",
        "\n",
        "        # LSTM output: output, (hidden, cell)\n",
        "        # We use the final hidden state or max pooling.\n",
        "        # Here we'll use the final hidden state of the last layer\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # Concat the final forward and backward hidden states\n",
        "        # hidden shape: [num_layers * num_directions, batch, hidden_dim]\n",
        "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "\n",
        "        dropped = self.dropout(hidden_cat)\n",
        "        return torch.sigmoid(self.fc(dropped))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d6735069",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6735069",
        "outputId": "fe1822df-a84b-4e4d-cd0b-b870e79da0c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Running Experiment: exp_3_bilstm (BiLSTM) ===\n",
            "Saving Model: True\n",
            "Experiment 'exp_3_bilstm' initialized at: /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_bilstm\n",
            "Mode: Incremental saving (5 folds)\n",
            "\n",
            "--- Fold 1/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.5033 - Val AUC: 0.7891\n",
            "Epoch 10/20 - Train Loss: 0.2809 - Val AUC: 0.9397\n",
            "Epoch 15/20 - Train Loss: 0.1689 - Val AUC: 0.9689\n",
            "Epoch 20/20 - Train Loss: 0.0983 - Val AUC: 0.9758\n",
            "Fold 1 Best Val AUC: 0.9758\n",
            "  Fold 1/5 saved | ROC AUC: 0.9758\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.2925 - Val AUC: 0.9262\n",
            "Epoch 10/20 - Train Loss: 0.2346 - Val AUC: 0.9487\n",
            "Epoch 15/20 - Train Loss: 0.1639 - Val AUC: 0.9620\n",
            "Epoch 20/20 - Train Loss: 0.1162 - Val AUC: 0.9679\n",
            "Fold 2 Best Val AUC: 0.9679\n",
            "  Fold 2/5 saved | ROC AUC: 0.9679\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.4129 - Val AUC: 0.8466\n",
            "Epoch 10/20 - Train Loss: 0.2513 - Val AUC: 0.9404\n",
            "Epoch 15/20 - Train Loss: 0.1703 - Val AUC: 0.9570\n",
            "Epoch 20/20 - Train Loss: 0.0880 - Val AUC: 0.9672\n",
            "Fold 3 Best Val AUC: 0.9672\n",
            "  Fold 3/5 saved | ROC AUC: 0.9672\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.2573 - Val AUC: 0.9506\n",
            "Epoch 10/20 - Train Loss: 0.1345 - Val AUC: 0.9678\n",
            "Epoch 15/20 - Train Loss: 0.0687 - Val AUC: 0.9647\n",
            "Epoch 20/20 - Train Loss: 0.0397 - Val AUC: 0.9695\n",
            "Fold 4 Best Val AUC: 0.9695\n",
            "  Fold 4/5 saved | ROC AUC: 0.9695\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.2950 - Val AUC: 0.9338\n",
            "Epoch 10/20 - Train Loss: 0.1644 - Val AUC: 0.9572\n",
            "Epoch 15/20 - Train Loss: 0.0754 - Val AUC: 0.9612\n",
            "Epoch 20/20 - Train Loss: 0.0380 - Val AUC: 0.9579\n",
            "Fold 5 Best Val AUC: 0.9579\n",
            "  Fold 5/5 saved | ROC AUC: 0.9579\n",
            "\n",
            "Finalizing experiment...\n",
            "  Predictions saved to /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_bilstm/exp_3_bilstm_prediction.csv\n",
            "\n",
            "✓ Experiment 'exp_3_bilstm' finalized!\n",
            "  Location: /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_bilstm\n",
            "  Folds completed: 5\n",
            "  Best fold: 1 (ROC AUC: 0.9758)\n",
            "  Average ROC AUC: 0.9677 ± 0.0057\n",
            "Experiment saved to /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_bilstm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(BiLSTM(\n",
              "   (embedding): Embedding(101, 64, padding_idx=0)\n",
              "   (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "   (dropout): Dropout(p=0.3, inplace=False)\n",
              "   (fc): Linear(in_features=256, out_features=1, bias=True)\n",
              " ),\n",
              " {'loss': 0.0766215846716647,\n",
              "  'accuracy': 0.9048140043763676,\n",
              "  'precision': 0.890295358649789,\n",
              "  'recall': 0.9234135667396062,\n",
              "  'f1': 0.9065520945220193,\n",
              "  'roc_auc': np.float64(0.9579205550421596),\n",
              "  'TP': np.int64(844),\n",
              "  'FP': np.int64(104),\n",
              "  'TN': np.int64(810),\n",
              "  'FN': np.int64(70)},\n",
              " {'loss': 0.31688410463504174,\n",
              "  'accuracy': 0.9151356080489939,\n",
              "  'precision': 0.8990748528174937,\n",
              "  'recall': 0.9352580927384077,\n",
              "  'f1': 0.9168096054888508,\n",
              "  'roc_auc': np.float64(0.968101701635502),\n",
              "  'TP': np.int64(1069),\n",
              "  'FP': np.int64(120),\n",
              "  'TN': np.int64(1023),\n",
              "  'FN': np.int64(74)})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run BiLSTM Experiment\n",
        "SAVE_LSTM = True # Set to False to skip saving\n",
        "\n",
        "lstm_params = {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 2,\n",
        "    'dropout': DROPOUT\n",
        "}\n",
        "\n",
        "run_experiment(BiLSTM, \"BiLSTM\", lstm_params, \"exp_3_bilstm\", save_model=SAVE_LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ad21eb",
      "metadata": {
        "id": "77ad21eb"
      },
      "source": [
        "### 4. Hybrid Model (Numeric + Text)\n",
        "\n",
        "From the above results, we see that CharCNN outperforms BiLSTM, therefore, we will choose that for the hybrid model, along with a parallel MLP to capture the numeric features that we engineered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7242542f",
      "metadata": {
        "id": "7242542f"
      },
      "outputs": [],
      "source": [
        "# Define Hybrid Model Architecture\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, vocab_size, numeric_input_dim, embedding_dim=64, num_filters=128, filter_sizes=[3, 4, 5], dropout=0.3):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # Text Branch (CNN)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_filters,\n",
        "                      kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.text_out_dim = len(filter_sizes) * num_filters\n",
        "\n",
        "        # Numeric Branch (MLP)\n",
        "        self.numeric_fc = nn.Sequential(\n",
        "            nn.Linear(numeric_input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.numeric_out_dim = 64\n",
        "\n",
        "        # Combined\n",
        "        self.fc_final = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.text_out_dim + self.numeric_out_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_numeric, x_text, x_img=None):\n",
        "        # Text Path\n",
        "        embedded = self.embedding(x_text).permute(0, 2, 1)\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        text_features = torch.cat(pooled, dim=1)\n",
        "\n",
        "        # Numeric Path\n",
        "        numeric_features = self.numeric_fc(x_numeric)\n",
        "\n",
        "        # Combine\n",
        "        combined = torch.cat((text_features, numeric_features), dim=1)\n",
        "        return torch.sigmoid(self.fc_final(combined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7ae7a0ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ae7a0ac",
        "outputId": "f3996309-2d15-404e-a48d-b399c9762071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Running Experiment: exp_3_hybrid (HybridModel) ===\n",
            "Saving Model: True\n",
            "Experiment 'exp_3_hybrid' initialized at: /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_hybrid\n",
            "Mode: Incremental saving (5 folds)\n",
            "\n",
            "--- Fold 1/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1205 - Val AUC: 0.9872\n",
            "Epoch 10/20 - Train Loss: 0.0648 - Val AUC: 0.9871\n",
            "Epoch 15/20 - Train Loss: 0.0353 - Val AUC: 0.9883\n",
            "Epoch 20/20 - Train Loss: 0.0351 - Val AUC: 0.9882\n",
            "Fold 1 Best Val AUC: 0.9882\n",
            "  Fold 1/5 saved | ROC AUC: 0.9882\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1125 - Val AUC: 0.9811\n",
            "Epoch 10/20 - Train Loss: 0.0678 - Val AUC: 0.9816\n",
            "Epoch 15/20 - Train Loss: 0.0383 - Val AUC: 0.9829\n",
            "Epoch 20/20 - Train Loss: 0.0248 - Val AUC: 0.9830\n",
            "Fold 2 Best Val AUC: 0.9830\n",
            "  Fold 2/5 saved | ROC AUC: 0.9830\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1264 - Val AUC: 0.9819\n",
            "Epoch 10/20 - Train Loss: 0.0778 - Val AUC: 0.9847\n",
            "Epoch 15/20 - Train Loss: 0.0464 - Val AUC: 0.9851\n",
            "Epoch 20/20 - Train Loss: 0.0311 - Val AUC: 0.9847\n",
            "Fold 3 Best Val AUC: 0.9847\n",
            "  Fold 3/5 saved | ROC AUC: 0.9847\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1216 - Val AUC: 0.9868\n",
            "Epoch 10/20 - Train Loss: 0.0719 - Val AUC: 0.9888\n",
            "Epoch 15/20 - Train Loss: 0.0347 - Val AUC: 0.9875\n",
            "Epoch 20/20 - Train Loss: 0.0296 - Val AUC: 0.9872\n",
            "Fold 4 Best Val AUC: 0.9872\n",
            "  Fold 4/5 saved | ROC AUC: 0.9872\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Epoch 5/20 - Train Loss: 0.1187 - Val AUC: 0.9816\n",
            "Epoch 10/20 - Train Loss: 0.0617 - Val AUC: 0.9823\n",
            "Epoch 15/20 - Train Loss: 0.0414 - Val AUC: 0.9830\n",
            "Epoch 20/20 - Train Loss: 0.0214 - Val AUC: 0.9831\n",
            "Fold 5 Best Val AUC: 0.9831\n",
            "  Fold 5/5 saved | ROC AUC: 0.9831\n",
            "\n",
            "Finalizing experiment...\n",
            "  Predictions saved to /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_hybrid/exp_3_hybrid_prediction.csv\n",
            "\n",
            "✓ Experiment 'exp_3_hybrid' finalized!\n",
            "  Location: /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_hybrid\n",
            "  Folds completed: 5\n",
            "  Best fold: 1 (ROC AUC: 0.9882)\n",
            "  Average ROC AUC: 0.9852 ± 0.0021\n",
            "Experiment saved to /content/drive/MyDrive/fraud-grp-proj/experiments/exp_3_hybrid\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(HybridModel(\n",
              "   (embedding): Embedding(101, 64, padding_idx=0)\n",
              "   (convs): ModuleList(\n",
              "     (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
              "     (1): Conv1d(64, 128, kernel_size=(4,), stride=(1,))\n",
              "     (2): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
              "   )\n",
              "   (numeric_fc): Sequential(\n",
              "     (0): Linear(in_features=16, out_features=128, bias=True)\n",
              "     (1): ReLU()\n",
              "     (2): Dropout(p=0.3, inplace=False)\n",
              "     (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "     (4): ReLU()\n",
              "   )\n",
              "   (fc_final): Sequential(\n",
              "     (0): Dropout(p=0.3, inplace=False)\n",
              "     (1): Linear(in_features=448, out_features=64, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=64, out_features=1, bias=True)\n",
              "   )\n",
              " ),\n",
              " {'loss': 0.045974995705826624,\n",
              "  'accuracy': 0.9442013129102844,\n",
              "  'precision': 0.953125,\n",
              "  'recall': 0.9343544857768052,\n",
              "  'f1': 0.943646408839779,\n",
              "  'roc_auc': np.float64(0.9830924495688272),\n",
              "  'TP': np.int64(854),\n",
              "  'FP': np.int64(42),\n",
              "  'TN': np.int64(872),\n",
              "  'FN': np.int64(60)},\n",
              " {'loss': 0.20686446859689522,\n",
              "  'accuracy': 0.9422572178477691,\n",
              "  'precision': 0.954995499549955,\n",
              "  'recall': 0.9282589676290464,\n",
              "  'f1': 0.9414374445430346,\n",
              "  'roc_auc': np.float64(0.9857682925242394),\n",
              "  'TP': np.int64(1061),\n",
              "  'FP': np.int64(50),\n",
              "  'TN': np.int64(1093),\n",
              "  'FN': np.int64(82)})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run Hybrid Model Experiment\n",
        "SAVE_HYBRID = True # Set to False to skip saving\n",
        "\n",
        "hybrid_params = {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'numeric_input_dim': X_numeric_train_scaled.shape[1],\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'num_filters': 128,\n",
        "    'filter_sizes': [3, 4, 5],\n",
        "    'dropout': DROPOUT\n",
        "}\n",
        "\n",
        "run_experiment(HybridModel, \"HybridModel\", hybrid_params, \"exp_3_hybrid\", save_model=SAVE_HYBRID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a74d029f",
      "metadata": {},
      "source": [
        "The hybrid model has the best fold performance so far, which tells us that adding in the numeric features does help improve performance.\n",
        "\n",
        "Although the hybrid model is the best performing neural network so far, we are unlikely able to Optuna tune it due to the complexity of the model and training time required. For the neural network models, only MLP would be feasible for Optuna tuning, but since it performed worst, we will not pursue that further."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
