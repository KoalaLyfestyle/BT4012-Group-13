{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92989aba",
   "metadata": {},
   "source": [
    "# Phishing URL Tree-Based Model Experiments\n",
    "\n",
    "This notebook explores various tree-based models using the Kaggle phishing URL dataset.\n",
    "\n",
    "In increasing order of complexity, we will experiment with:\n",
    "\n",
    "1. Random Forest\n",
    "2. XGBoost\n",
    "3. LightGBM\n",
    "4. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-imports",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix,\n",
    "                             classification_report)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Tree-based models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Import ModelSaver\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "from save_model import ModelSaver\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAVE_MODELS = True\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check for Google Drive (if running in Colab)\n",
    "use_drive = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    use_drive = True\n",
    "    drive_root = '/content/drive/MyDrive/fraud-grp-proj/'\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "train_w_features_df = pd.read_csv('dataset/df_train_feature_engineered.csv')\n",
    "test_w_features_df = pd.read_csv('dataset/df_test_feature_engineered.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "print(f\"Train with features shape: {train_w_features_df.shape}\")\n",
    "print(f\"Test with features shape: {test_w_features_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing",
   "metadata": {},
   "source": [
    "Due to the robust nature of tree-based models, we will just be using the full feature set including originals and transformed features, unlike our approach for linear and neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original versions of log transformed features\n",
    "train_w_features_df.drop(columns=['length_url', 'length_path',  'ratio_hostname_url', 'length_words_url', 'avg_word_hostname', 'num_unique_chars_hostname'], inplace=True)\n",
    "\n",
    "# Drop original versions of squared transformed features\n",
    "train_w_features_df.drop(columns=['ratio_letter_url', 'entropy_hostname'], inplace=True)\n",
    "\n",
    "# Drop original versions of is_zero transformed features\n",
    "train_w_features_df.drop(columns=['num_hyphens_domain', 'length_subdomains', 'num_hyphens',  'num_at', 'num_question_marks', 'num_and', 'num_equal', 'num_percent', 'ratio_digits_url', 'ratio_digits_hostname', 'avg_word_path', 'length_query'], inplace=True)\n",
    "\n",
    "# Drop original versions of bucketed transformed features\n",
    "train_w_features_df.drop(columns=['num_subdomain', 'length_tld', 'path_depth'], inplace=True)\n",
    "\n",
    "# Prepare X and y\n",
    "numeric_cols = train_w_features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'target' in numeric_cols:\n",
    "    numeric_cols.remove('target')\n",
    "\n",
    "X = train_w_features_df[numeric_cols].values\n",
    "y = train_w_features_df['target'].values\n",
    "\n",
    "X_test = test_w_features_df[numeric_cols].values\n",
    "if 'target' in test_w_features_df.columns:\n",
    "    y_test = test_w_features_df['target'].values\n",
    "else:\n",
    "    y_test = np.zeros(len(test_w_features_df))\n",
    "\n",
    "print(f\"Features used: {len(numeric_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-setup",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "Now lets move on to training the models. We use the `ModelSaver` utility to help us standardize the storing of metrics and models for evaluation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tree_experiment(model_class, model_name, model_params, experiment_name, save_model=True):\n",
    "    print(f\"\\n=== Running Experiment: {experiment_name} ({model_name}) ===\")\n",
    "    print(f\"Saving Model: {save_model}\")\n",
    "\n",
    "    saver = None\n",
    "    if save_model:\n",
    "        if use_drive:\n",
    "            base_path = drive_root + \"experiments\"\n",
    "        else:\n",
    "            base_path = \"experiments\"\n",
    "        saver = ModelSaver(base_path=base_path)\n",
    "        saver.start_experiment(\n",
    "            experiment_name=experiment_name,\n",
    "            model_type=model_name,\n",
    "            vectorizer=\"None (Numeric Features)\",\n",
    "            vectorizer_params={},\n",
    "            model_params=model_params,\n",
    "            n_folds=5,\n",
    "            save_format=\"pickle\"\n",
    "        )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_test_preds = []\n",
    "    \n",
    "    # Store feature names for importance analysis\n",
    "    feature_names = numeric_cols\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        print(f\"\\n--- Fold {fold}/5 ---\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Initialize and train model\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Validation predictions\n",
    "        val_probs = model.predict_proba(X_val)[:, 1]\n",
    "        val_preds = (val_probs > 0.5).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, val_preds).ravel()\n",
    "        \n",
    "        metrics = {\n",
    "            'fold': fold,\n",
    "            'accuracy': accuracy_score(y_val, val_preds),\n",
    "            'precision': precision_score(y_val, val_preds, zero_division=0),\n",
    "            'recall': recall_score(y_val, val_preds, zero_division=0),\n",
    "            'f1': f1_score(y_val, val_preds, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_val, val_probs),\n",
    "            'TP': int(tp),\n",
    "            'FP': int(fp),\n",
    "            'TN': int(tn),\n",
    "            'FN': int(fn),\n",
    "            'train_size': len(train_idx),\n",
    "            'val_size': len(val_idx)\n",
    "        }\n",
    "        \n",
    "        print(f\"Fold {fold} Val AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "        # Test predictions\n",
    "        test_probs = model.predict_proba(X_test)[:, 1]\n",
    "        fold_test_preds.append(test_probs)\n",
    "\n",
    "        if save_model and saver:\n",
    "            saver.add_fold(\n",
    "                fold_model=model,\n",
    "                fold_metric=metrics,\n",
    "                test_predictions=test_probs,\n",
    "                feature_names=feature_names\n",
    "            )\n",
    "\n",
    "    if save_model and saver:\n",
    "        saver.finalize_experiment()\n",
    "        print(f\"Experiment saved to {saver._exp_dir}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf-experiment",
   "metadata": {},
   "source": [
    "### 1. Random Forest\n",
    "\n",
    "We use the hyperparameters identified in previous experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt',\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "run_tree_experiment(RandomForestClassifier, \"RandomForest\", rf_params, \"exp_2_random_forest\", save_model=SAVE_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-experiment",
   "metadata": {},
   "source": [
    "### 2. XGBoost\n",
    "\n",
    "We use the optimized hyperparameters from the legacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': 400,\n",
    "    'learning_rate': 0.01818965322291987,\n",
    "    'max_depth': 12,\n",
    "    'subsample': 0.9089044013517584,\n",
    "    'colsample_bytree': 0.5968303772495912,\n",
    "    'reg_alpha': 0.34341485605720035,\n",
    "    'reg_lambda': 1.7747160863049662,\n",
    "    'min_child_weight': 4,\n",
    "    'gamma': 0.5508393571724655,\n",
    "    'random_state': SEED,\n",
    "    'eval_metric': \"logloss\",\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "run_tree_experiment(XGBClassifier, \"XGBoost\", xgb_params, \"exp_2_xgboost\", save_model=SAVE_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-experiment",
   "metadata": {},
   "source": [
    "### 3. LightGBM\n",
    "\n",
    "We use the optimized hyperparameters for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgbm-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 10,\n",
    "    'num_leaves': 31,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_samples': 20,\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "run_tree_experiment(LGBMClassifier, \"LightGBM\", lgbm_params, \"exp_2_lightgbm\", save_model=SAVE_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catboost-experiment",
   "metadata": {},
   "source": [
    "### 4. CatBoost\n",
    "\n",
    "We use the optimized hyperparameters for CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catboost-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'iterations': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'subsample': 0.8,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'eval_metric': 'AUC',\n",
    "    'task_type': 'CPU'\n",
    "}\n",
    "\n",
    "run_tree_experiment(CatBoostClassifier, \"CatBoost\", catboost_params, \"exp_2_catboost\", save_model=SAVE_MODELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
