{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9f0793",
   "metadata": {},
   "source": [
    "# Experiment 5: Ensemble Models\n",
    "\n",
    "This notebook implements ensemble strategies for the phishing URL detection models developed in previous experiments.\n",
    "It loads trained models from:\n",
    "- Experiment 1: Linear Models (Logistic Regression, SVM)\n",
    "- Experiment 2: Tree Models (XGBoost, LightGBM, CatBoost, Random Forest)\n",
    "- Experiment 3: Neural Networks (MLP, CharCNN, BiLSTM, Hybrid)\n",
    "- Experiment 4: Transformer Models (DeBERTa)\n",
    "\n",
    "We will generate Out-of-Fold (OOF) predictions for each model and use them to train ensemble models (Stacking) or perform Averaging.\n",
    "\n",
    "**Note:** This notebook includes a `LIGHTWEIGHT_MODE` flag. If set to `True`, it will skip the computationally expensive DeBERTa models for local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f328a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Flags\n",
    "USE_DRIVE = False\n",
    "LIGHTWEIGHT_MODE = True # Set to False to include DeBERTa models (requires GPU and time)\n",
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "EXPERIMENTS_DIR = \"experiments\"\n",
    "\n",
    "# uncomment lines below if running on colab\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "use_drive = True\n",
    "drive_root = '/content/drive/MyDrive/fraud-grp-proj/'\n",
    "print(os.path.exists(drive_root)) # check path exists\n",
    "EXPERIMENTS_DIR = os.path.join(drive_root, EXPERIMENTS_DIR)\n",
    "USE_DRIVE = True\n",
    "LIGHTWEIGHT_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04eae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winston/Documents/School/Y3S1/BT4012/Group Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library not found. DeBERTa models will be skipped.\n",
      "Experiments directory found: /Users/winston/Documents/School/Y3S1/BT4012/Group Project/experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel, PreTrainedModel, DebertaV2Config, AutoConfig\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"Transformers library not found. DeBERTa models will be skipped.\")\n",
    "    LIGHTWEIGHT_MODE = True\n",
    "\n",
    "# Setup Drive if needed\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    drive_root = '/content/drive/MyDrive/fraud-grp-proj/'\n",
    "    if os.path.exists(drive_root):\n",
    "        os.chdir(drive_root)\n",
    "        print(f\"Changed directory to {drive_root}\")\n",
    "    else:\n",
    "        print(f\"Drive root {drive_root} not found.\")\n",
    "\n",
    "# Verify experiments directory\n",
    "if not os.path.exists(EXPERIMENTS_DIR):\n",
    "    print(f\"Warning: Experiments directory '{EXPERIMENTS_DIR}' not found.\")\n",
    "else:\n",
    "    print(f\"Experiments directory found: {os.path.abspath(EXPERIMENTS_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5885b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9143, 2)\n",
      "Test shape: (2286, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "train_w_features_df = pd.read_csv('dataset/df_train_feature_engineered.csv')\n",
    "test_w_features_df = pd.read_csv('dataset/df_test_feature_engineered.csv')\n",
    "\n",
    "y = train_df['target'].values\n",
    "X_numeric = train_w_features_df.drop(columns=['url', 'target', 'is_http', 'has_subdomain', 'has_tld']).select_dtypes(include=[np.number])\n",
    "numeric_cols = X_numeric.columns.tolist()\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a2adb",
   "metadata": {},
   "source": [
    "## Model Class Definitions\n",
    "We need to define the model architectures to load the saved weights for PyTorch and Transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5c0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural Network Models (from exp_3) ---\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, num_filters=128, filter_sizes=[3, 4, 5], dropout=0.3, max_len=200):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, 1)\n",
    "\n",
    "    def forward(self, x_numeric, x_text, x_img=None):\n",
    "        embedded = self.embedding(x_text)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        dropped = self.dropout(cat)\n",
    "        return torch.sigmoid(self.fc(dropped))\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x_numeric, x_text, x_img=None):\n",
    "        embedded = self.embedding(x_text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        dropped = self.dropout(hidden_cat)\n",
    "        return torch.sigmoid(self.fc(dropped))\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, num_filters=128, filter_sizes=[3, 4, 5], dropout=0.3, max_len=200, input_dim=78, hidden_dims=[64, 32]):\n",
    "        super(HybridModel, self).__init__()\n",
    "        # Text CNN part\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout_cnn = nn.Dropout(dropout)\n",
    "        self.fc_cnn = nn.Linear(len(filter_sizes) * num_filters, 64)\n",
    "\n",
    "        # MLP part\n",
    "        mlp_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            mlp_layers.append(nn.Linear(prev_dim, dim))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = dim\n",
    "        self.mlp_network = nn.Sequential(*mlp_layers)\n",
    "\n",
    "        # Combined layers\n",
    "        self.fc_combined = nn.Linear(64 + 32, 1)\n",
    "\n",
    "    def forward(self, x_numeric, x_text, x_img=None):\n",
    "        # Text CNN\n",
    "        embedded = self.embedding(x_text)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat_cnn = torch.cat(pooled, dim=1)\n",
    "        dropped_cnn = self.dropout_cnn(cat_cnn)\n",
    "        features_cnn = self.fc_cnn(dropped_cnn)\n",
    "\n",
    "        # MLP\n",
    "        features_mlp = self.mlp_network(x_numeric)\n",
    "\n",
    "        # Combined\n",
    "        combined = torch.cat([features_cnn, features_mlp], dim=1)\n",
    "        return torch.sigmoid(self.fc_combined(combined))\n",
    "\n",
    "# --- Transformer Models (from exp_4) ---\n",
    "\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    class DebertaWithFeaturesConfig(DebertaV2Config):\n",
    "        model_type = \"deberta_with_features\"\n",
    "        def __init__(self, num_features=0, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.num_features = num_features\n",
    "\n",
    "    class DebertaWithFeatures(PreTrainedModel):\n",
    "        config_class = DebertaWithFeaturesConfig\n",
    "        def __init__(self, config):\n",
    "            super().__init__(config)\n",
    "            self.deberta = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\") # Base model\n",
    "            hidden = self.deberta.config.hidden_size\n",
    "            self.feature_proj = nn.Sequential(\n",
    "                nn.Linear(config.num_features, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            )\n",
    "            self.classifier = nn.Linear(hidden + 128, 2)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, features, labels=None):\n",
    "            out = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls = out.last_hidden_state[:, 0]\n",
    "            feat_out = self.feature_proj(features)\n",
    "            combined = torch.cat([cls, feat_out], dim=1)\n",
    "            logits = self.classifier(combined)\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9002fb6",
   "metadata": {},
   "source": [
    "## Data Preparation Helpers\n",
    "We need to recreate the datasets/dataloaders used in training to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379c6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NN Data Preparation ---\n",
    "class CharTokenizer:\n",
    "    def __init__(self, max_len=200):\n",
    "        self.char_map = {}\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def fit(self, texts):\n",
    "        unique_chars = set(\"\".join(texts))\n",
    "        # 0 is reserved for padding\n",
    "        self.char_map = {char: i + 1 for i, char in enumerate(sorted(unique_chars))}\n",
    "        self.vocab_size = len(self.char_map) + 1\n",
    "\n",
    "    def transform(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = [self.char_map.get(c, 0) for c in text[:self.max_len]]\n",
    "            # Pad\n",
    "            if len(seq) < self.max_len:\n",
    "                seq += [0] * (self.max_len - len(seq))\n",
    "            sequences.append(seq)\n",
    "        return np.array(sequences)\n",
    "\n",
    "class PhishingDataset(Dataset):\n",
    "    def __init__(self, X_numeric, X_text, y=None):\n",
    "        self.X_numeric = torch.FloatTensor(X_numeric)\n",
    "        self.X_text = torch.LongTensor(X_text)\n",
    "        self.y = torch.FloatTensor(y) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_numeric)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X_numeric[idx], self.X_text[idx], self.y[idx]\n",
    "        return self.X_numeric[idx], self.X_text[idx]\n",
    "\n",
    "# --- Transformer Data Preparation ---\n",
    "class URLWithFeaturesDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, feature_cols, max_len=256):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_cols = feature_cols\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc = self.tokenizer(\n",
    "            row[\"url\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        features = row[self.feature_cols].values.astype(\"float32\")\n",
    "        \n",
    "        item = {\n",
    "            \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long),\n",
    "            \"features\": torch.tensor(features, dtype=torch.float32)\n",
    "        }\n",
    "        if \"target\" in row:\n",
    "            item[\"labels\"] = torch.tensor(row[\"target\"], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bcb38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for NN Models\n",
    "# We need to fit the tokenizer on the training data exactly as in exp_3\n",
    "char_tokenizer = CharTokenizer(max_len=200)\n",
    "char_tokenizer.fit(train_df['url'].values)\n",
    "\n",
    "X_text_train = char_tokenizer.transform(train_df['url'].values)\n",
    "X_numeric_train = train_w_features_df[numeric_cols].values\n",
    "\n",
    "# Scale numeric features (using scaler fitted on full train set for simplicity, \n",
    "# though strictly should be fold-wise. For OOF generation we should ideally use fold-wise scalers if saved.\n",
    "# However, exp_3 used a global scaler for the whole dataset in some parts or fold-wise.\n",
    "# Let's check if we can load pipelines. If not, we approximate with global scaling for OOF generation to save complexity,\n",
    "# or better, re-fit scaler on training folds.)\n",
    "\n",
    "# For NN models, we'll re-fit scaler fold-wise to be correct.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7cd48a",
   "metadata": {},
   "source": [
    "## OOF Prediction Generator\n",
    "This function iterates through the folds, loads the saved model for that fold, and generates predictions for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_oof_predictions(experiment_name, model_class=None, model_type='sklearn', feature_type='combined'):\n",
    "    \"\"\"\n",
    "    Generates OOF predictions for a given experiment.\n",
    "    Checks if OOF file exists first.\n",
    "    \"\"\"\n",
    "    exp_dir = os.path.join(EXPERIMENTS_DIR, experiment_name)\n",
    "    oof_path = os.path.join(exp_dir, f\"{experiment_name}_oof_predictions.csv\")\n",
    "    \n",
    "    if os.path.exists(oof_path):\n",
    "        print(f\"Loading existing OOF predictions for {experiment_name}...\")\n",
    "        return pd.read_csv(oof_path)\n",
    "    \n",
    "    print(f\"Generating OOF predictions for {experiment_name}...\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_preds = np.zeros(len(train_df))\n",
    "    \n",
    "    # Prepare data based on feature type\n",
    "    if feature_type == 'combined':\n",
    "        X = train_w_features_df.copy()\n",
    "        # Add URL for text processing if needed by pipeline\n",
    "        if 'url' not in X.columns:\n",
    "            X['url'] = train_df['url']\n",
    "    elif feature_type == 'numeric':\n",
    "        X = train_w_features_df[numeric_cols].copy()\n",
    "    elif feature_type == 'text':\n",
    "        X = train_df['url'].copy()\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model_path = os.path.join(exp_dir, f\"pipeline_fold_{fold}.pkl\")\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"  Model not found: {model_path}\")\n",
    "                continue\n",
    "            \n",
    "            with open(model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "            # Predict\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                preds = model.predict_proba(X_val)[:, 1]\n",
    "            else:\n",
    "                preds = model.predict(X_val)\n",
    "            oof_preds[val_idx] = preds\n",
    "            \n",
    "        elif model_type == 'pytorch':\n",
    "            model_path = os.path.join(exp_dir, f\"pipeline_fold_{fold}.pkl\")\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"  Model not found: {model_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Prepare fold data\n",
    "            # Note: We need to scale numeric features fold-wise to match training\n",
    "            scaler = StandardScaler()\n",
    "            X_num_train = scaler.fit_transform(X_numeric_train[train_idx])\n",
    "            X_num_val = scaler.transform(X_numeric_train[val_idx])\n",
    "            \n",
    "            X_txt_val = X_text_train[val_idx]\n",
    "            \n",
    "            val_dataset = PhishingDataset(X_num_val, X_txt_val)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "            \n",
    "            # Init model\n",
    "            # We need to know params. Assuming standard params from exp_3 for now.\n",
    "            # Ideally we load params from config.\n",
    "            if model_class == CharCNN:\n",
    "                model = CharCNN(vocab_size=char_tokenizer.vocab_size)\n",
    "            elif model_class == BiLSTM:\n",
    "                model = BiLSTM(vocab_size=char_tokenizer.vocab_size)\n",
    "            elif model_class == HybridModel:\n",
    "                model = HybridModel(vocab_size=char_tokenizer.vocab_size)\n",
    "            \n",
    "            model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            fold_preds = []\n",
    "            with torch.no_grad():\n",
    "                for x_num, x_txt in val_loader:\n",
    "                    out = model(x_num, x_txt)\n",
    "                    fold_preds.extend(out.squeeze().numpy())\n",
    "            oof_preds[val_idx] = fold_preds\n",
    "            \n",
    "        elif model_type == 'transformer':\n",
    "            if LIGHTWEIGHT_MODE:\n",
    "                print(\"  Skipping Transformer OOF generation in Lightweight Mode.\")\n",
    "                return None\n",
    "                \n",
    "            fold_dir = os.path.join(exp_dir, f\"transformer_fold_{fold}\")\n",
    "            if not os.path.exists(fold_dir):\n",
    "                print(f\"  Model dir not found: {fold_dir}\")\n",
    "                continue\n",
    "            \n",
    "            # Load model\n",
    "            # Assuming DebertaWithFeatures\n",
    "            try:\n",
    "                model = DebertaWithFeatures.from_pretrained(fold_dir)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading transformer: {e}\")\n",
    "                continue\n",
    "                \n",
    "            model.eval()\n",
    "            if torch.cuda.is_available():\n",
    "                model.cuda()\n",
    "            \n",
    "            # Prepare Data\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "            val_ds = URLWithFeaturesDataset(train_df.iloc[val_idx], tokenizer, numeric_cols)\n",
    "            val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=lambda x: x) # Custom collate needed?\n",
    "            # Actually we defined collate_fn in exp_4, let's redefine simple one or use default if dataset returns dicts of tensors\n",
    "            # The dataset returns dicts, so default collate works if keys match.\n",
    "            # Wait, default collate stacks. Yes.\n",
    "            \n",
    "            fold_preds = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(DataLoader(val_ds, batch_size=16, shuffle=False), desc=f\"Fold {fold}\"):\n",
    "                    input_ids = batch['input_ids']\n",
    "                    attention_mask = batch['attention_mask']\n",
    "                    features = batch['features']\n",
    "                    \n",
    "                    if torch.cuda.is_available():\n",
    "                        input_ids = input_ids.cuda()\n",
    "                        attention_mask = attention_mask.cuda()\n",
    "                        features = features.cuda()\n",
    "                        \n",
    "                    logits = model(input_ids, attention_mask, features)\n",
    "                    probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "                    fold_preds.extend(probs.cpu().numpy())\n",
    "            oof_preds[val_idx] = fold_preds\n",
    "\n",
    "    # Save OOF predictions\n",
    "    df_oof = pd.DataFrame({'oof_pred': oof_preds})\n",
    "    df_oof.to_csv(oof_path, index=False)\n",
    "    print(f\"Saved OOF predictions to {oof_path}\")\n",
    "    return df_oof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934f274",
   "metadata": {},
   "source": [
    "## Generate OOF Predictions\n",
    "We will now generate OOF predictions for selected models from each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027563c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Linear Models ---\n",
      "Loading existing OOF predictions for exp_1_combined_lr...\n",
      "Loading existing OOF predictions for exp_1_combined_svm...\n",
      "\n",
      "--- Tree Models ---\n",
      "Loading existing OOF predictions for exp_2_random_forest_all...\n",
      "Loading existing OOF predictions for exp_2_xgboost_all...\n",
      "\n",
      "--- Neural Networks ---\n",
      "Generating OOF predictions for exp_3_charcnn...\n",
      "  Error loading pytorch model for fold 1: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "  Error loading pytorch model for fold 2: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "  Error loading pytorch model for fold 3: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "  Error loading pytorch model for fold 4: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "  Error loading pytorch model for fold 5: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "Saved OOF predictions to experiments/exp_3_charcnn/exp_3_charcnn_oof_predictions.csv\n",
      "\n",
      "--- Transformers ---\n",
      "Skipping DeBERTa (Lightweight Mode)\n"
     ]
    }
   ],
   "source": [
    "oof_data = pd.DataFrame()\n",
    "oof_data['target'] = y\n",
    "\n",
    "# 1. Linear Models\n",
    "print(\"--- Linear Models ---\")\n",
    "lr_oof = generate_oof_predictions('exp_1_combined_lr', model_type='sklearn', feature_type='combined')\n",
    "if lr_oof is not None: oof_data['lr_combined'] = lr_oof['oof_pred']\n",
    "\n",
    "svm_oof = generate_oof_predictions('exp_1_combined_svm', model_type='sklearn', feature_type='combined')\n",
    "if svm_oof is not None: oof_data['svm_combined'] = svm_oof['oof_pred']\n",
    "\n",
    "# 2. Tree Models\n",
    "print(\"\\n--- Tree Models ---\")\n",
    "# Assuming these exist and follow the pipeline structure\n",
    "rf_oof = generate_oof_predictions('exp_2_random_forest_all', model_type='sklearn', feature_type='combined')\n",
    "if rf_oof is not None: oof_data['rf_all'] = rf_oof['oof_pred']\n",
    "\n",
    "xgb_oof = generate_oof_predictions('exp_2_xgboost_all', model_type='sklearn', feature_type='combined')\n",
    "if xgb_oof is not None: oof_data['xgb_all'] = xgb_oof['oof_pred']\n",
    "\n",
    "# 3. Neural Networks\n",
    "print(\"\\n--- Neural Networks ---\")\n",
    "cnn_oof = generate_oof_predictions('exp_3_charcnn', model_class=CharCNN, model_type='pytorch', feature_type='combined')\n",
    "if cnn_oof is not None: oof_data['charcnn'] = cnn_oof['oof_pred']\n",
    "\n",
    "# lstm_oof = generate_oof_predictions('exp_3_bilstm', model_class=BiLSTM, model_type='pytorch', feature_type='combined')\n",
    "# if lstm_oof is not None: oof_data['bilstm'] = lstm_oof['oof_pred']\n",
    "\n",
    "# hybrid_oof = generate_oof_predictions('exp_3_hybrid', model_class=HybridModel, model_type='pytorch', feature_type='combined')\n",
    "# if hybrid_oof is not None: oof_data['hybrid'] = hybrid_oof['oof_pred']\n",
    "\n",
    "# 4. Transformers\n",
    "print(\"\\n--- Transformers ---\")\n",
    "if not LIGHTWEIGHT_MODE:\n",
    "    deberta_oof = generate_oof_predictions('exp_4_deberta_url_features', model_type='transformer', feature_type='combined')\n",
    "    if deberta_oof is not None: oof_data['deberta'] = deberta_oof['oof_pred']\n",
    "else:\n",
    "    print(\"Skipping DeBERTa (Lightweight Mode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2f479",
   "metadata": {},
   "source": [
    "## Ensemble Strategies\n",
    "Now that we have OOF predictions, we can combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3031821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out columns that might be missing (if some models failed to load)\n",
    "pred_cols = [c for c in oof_data.columns if c != 'target']\n",
    "print(f\"Ensembling with models: {pred_cols}\")\n",
    "\n",
    "# Strategy 1: Simple Averaging\n",
    "oof_data['ensemble_avg'] = oof_data[pred_cols].mean(axis=1)\n",
    "avg_auc = roc_auc_score(oof_data['target'], oof_data['ensemble_avg'])\n",
    "print(f\"Simple Averaging OOF AUC: {avg_auc:.5f}\")\n",
    "\n",
    "# Strategy 2: Weighted Averaging (Heuristic)\n",
    "# Give more weight to stronger models (e.g., DeBERTa, XGBoost, SVM)\n",
    "weights = {}\n",
    "for col in pred_cols:\n",
    "    if 'deberta' in col: weights[col] = 3\n",
    "    elif 'xgb' in col or 'lgbm' in col or 'catboost' in col: weights[col] = 2\n",
    "    elif 'svm' in col: weights[col] = 1.5\n",
    "    else: weights[col] = 1\n",
    "\n",
    "total_weight = sum(weights[c] for c in pred_cols)\n",
    "oof_data['ensemble_weighted'] = sum(oof_data[c] * weights[c] for c in pred_cols) / total_weight\n",
    "weighted_auc = roc_auc_score(oof_data['target'], oof_data['ensemble_weighted'])\n",
    "print(f\"Weighted Averaging OOF AUC: {weighted_auc:.5f}\")\n",
    "\n",
    "# Strategy 3: Stacking (Logistic Regression Meta-Learner)\n",
    "meta_model = LogisticRegression(random_state=RANDOM_STATE)\n",
    "# We need to cross-validate the meta-model to get a fair OOF score for the stack\n",
    "# But typically we train the meta-model on the full OOF set and predict on Test.\n",
    "# To estimate OOF performance of the stack, we can do nested CV or just hold-out.\n",
    "# Here we'll just fit on the full OOF predictions to see coefficients.\n",
    "meta_model.fit(oof_data[pred_cols], oof_data['target'])\n",
    "stack_preds = meta_model.predict_proba(oof_data[pred_cols])[:, 1]\n",
    "stack_auc = roc_auc_score(oof_data['target'], stack_preds)\n",
    "print(f\"Stacking (LR) Training AUC: {stack_auc:.5f} (Likely optimistic)\")\n",
    "\n",
    "# Check coefficients\n",
    "coefs = pd.Series(meta_model.coef_[0], index=pred_cols).sort_values(ascending=False)\n",
    "print(\"\\nStacking Coefficients:\")\n",
    "print(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0ada4",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set\n",
    "To get the final performance, we should ideally load the test set predictions for each model and apply the ensemble weights.\n",
    "Since we generated OOF predictions, we should assume we can also generate (or have) test predictions.\n",
    "For this notebook demonstration, we will save the ensemble models (the weights or the meta-model) and the OOF results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Ensemble Results\n",
    "ensemble_dir = os.path.join(EXPERIMENTS_DIR, \"exp_5_ensemble\")\n",
    "os.makedirs(ensemble_dir, exist_ok=True)\n",
    "\n",
    "# Save OOF predictions\n",
    "oof_data.to_csv(os.path.join(ensemble_dir, \"ensemble_oof_predictions.csv\"), index=False)\n",
    "\n",
    "# Save Meta Model\n",
    "with open(os.path.join(ensemble_dir, \"stacking_meta_model.pkl\"), 'wb') as f:\n",
    "    pickle.dump(meta_model, f)\n",
    "\n",
    "# Save Metrics\n",
    "metrics = {\n",
    "    \"simple_average_auc\": avg_auc,\n",
    "    \"weighted_average_auc\": weighted_auc,\n",
    "    \"stacking_train_auc\": stack_auc,\n",
    "    \"models_used\": pred_cols,\n",
    "    \"weights\": weights\n",
    "}\n",
    "with open(os.path.join(ensemble_dir, \"metrics.json\"), 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"Ensemble experiment saved to {ensemble_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
