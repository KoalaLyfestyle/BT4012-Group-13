{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison\n",
    "\n",
    "This notebook aggregates results from all experiments, compares their performance metrics (CV and Test), and visualizes key insights such as feature importance and cross-validation stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score, f1_score, log_loss\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment groups\n",
    "EXPERIMENTS = {\n",
    "    \"Linear\": [\n",
    "        \"exp_1_numeric_lr\",\n",
    "        \"exp_1_tfidf_lr\",\n",
    "        \"exp_1_combined_lr\",\n",
    "        \"exp_1_combined_svm\"\n",
    "    ],\n",
    "    \"Tree\": [\n",
    "        \"exp_2_random_forest_numeric\",\n",
    "        \"exp_2_random_forest_all\",\n",
    "        \"exp_2_xgboost_all\",\n",
    "        \"exp_2_lgbm_all\",\n",
    "        \"exp_2_catboost_all\",\n",
    "        \"exp_2_catboost_optuna\"\n",
    "    ],\n",
    "    \"Neural Network\": [\n",
    "        \"exp_3_mlp_baseline\",\n",
    "        \"exp_3_charcnn\",\n",
    "        \"exp_3_bilstm\",\n",
    "        \"exp_3_hybrid\",\n",
    "        \"exp_3_visual_cnn\"\n",
    "    ],\n",
    "    \"Transformer\": [\n",
    "        \"exp_4_deberta_url_only\",\n",
    "        \"exp_4_deberta_url_features\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "BASE_PATH = Path(\"experiments\")\n",
    "TEST_DATA_PATH = Path(\"dataset/df_test_feature_engineered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(exp_name):\n",
    "    metrics_path = BASE_PATH / exp_name / \"metrics.json\"\n",
    "    if not metrics_path.exists():\n",
    "        print(f\"Warning: Metrics file not found for {exp_name}\")\n",
    "        return None\n",
    "    \n",
    "    with open(metrics_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_test_labels():\n",
    "    if not TEST_DATA_PATH.exists():\n",
    "        print(f\"Warning: Test data not found at {TEST_DATA_PATH}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(TEST_DATA_PATH)\n",
    "        if \"target\" in df.columns:\n",
    "            return df[\"target\"].values\n",
    "        elif \"label\" in df.columns:\n",
    "            return df[\"label\"].values\n",
    "        else:\n",
    "            print(\"Error: Could not find target/label column in test data\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading test data: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_test_set(exp_name, y_true):\n",
    "    pred_path = BASE_PATH / exp_name / f\"{exp_name}_prediction.csv\"\n",
    "    if not pred_path.exists():\n",
    "        print(f\"Warning: Prediction file not found for {exp_name}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        preds_df = pd.read_csv(pred_path)\n",
    "        \n",
    "        # Extract probabilities\n",
    "        if \"probability\" in preds_df.columns:\n",
    "            y_scores = preds_df[\"probability\"].astype(float).values\n",
    "        elif \"proba\" in preds_df.columns:\n",
    "            y_scores = preds_df[\"proba\"].astype(float).values\n",
    "        elif preds_df.shape[1] >= 2:\n",
    "            y_scores = preds_df.iloc[:, -1].astype(float).values\n",
    "        else:\n",
    "            print(f\"Error: Could not locate probability column for {exp_name}\")\n",
    "            return None\n",
    "            \n",
    "        # Ensure lengths match\n",
    "        if len(y_scores) != len(y_true):\n",
    "             min_len = min(len(y_scores), len(y_true))\n",
    "             y_scores = y_scores[:min_len]\n",
    "             y_true_eval = y_true[:min_len]\n",
    "        else:\n",
    "            y_true_eval = y_true\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {}\n",
    "        metrics[\"roc_auc\"] = roc_auc_score(y_true_eval, y_scores)\n",
    "        metrics[\"log_loss\"] = log_loss(y_true_eval, y_scores)\n",
    "        \n",
    "        # Threshold metrics (0.5)\n",
    "        y_pred = (y_scores > 0.5).astype(int)\n",
    "        metrics[\"accuracy\"] = accuracy_score(y_true_eval, y_pred)\n",
    "        metrics[\"recall\"] = recall_score(y_true_eval, y_pred)\n",
    "        metrics[\"precision\"] = precision_score(y_true_eval, y_pred)\n",
    "        metrics[\"f1\"] = f1_score(y_true_eval, y_pred)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {exp_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Metrics Comparison Table\n",
    "\n",
    "We aggregate the average cross-validation metrics and calculate test set metrics for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_table():\n",
    "    y_true = get_test_labels()\n",
    "    if y_true is None:\n",
    "        print(\"Skipping test metrics due to missing labels\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for group, exp_list in EXPERIMENTS.items():\n",
    "        for exp_name in exp_list:\n",
    "            data = load_metrics(exp_name)\n",
    "            if not data:\n",
    "                continue\n",
    "                \n",
    "            avg_metrics = data.get(\"average_metrics\", {})\n",
    "            \n",
    "            # Get test metrics\n",
    "            test_metrics = {}\n",
    "            if y_true is not None:\n",
    "                test_metrics = evaluate_test_set(exp_name, y_true) or {}\n",
    "            \n",
    "            # Basic info\n",
    "            row = {\n",
    "                \"Group\": group,\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Model\": data.get(\"model_type\", \"N/A\"),\n",
    "                \"Vectorizer\": data.get(\"vectorizer\", \"N/A\"),\n",
    "                \n",
    "                # CV Metrics\n",
    "                \"CV ROC AUC\": f\"{avg_metrics.get('roc_auc', 0):.4f} ± {avg_metrics.get('roc_auc_std', 0):.4f}\",\n",
    "                \"CV Recall\": f\"{avg_metrics.get('recall', 0):.4f} ± {avg_metrics.get('recall_std', 0):.4f}\",\n",
    "                \"CV F1\": f\"{avg_metrics.get('f1', 0):.4f} ± {avg_metrics.get('f1_std', 0):.4f}\",\n",
    "                \n",
    "                # Test Metrics\n",
    "                \"Test ROC AUC\": f\"{test_metrics.get('roc_auc', 0):.4f}\" if test_metrics else \"N/A\",\n",
    "                \"Test Recall\": f\"{test_metrics.get('recall', 0):.4f}\" if test_metrics else \"N/A\",\n",
    "                \"Test F1\": f\"{test_metrics.get('f1', 0):.4f}\" if test_metrics else \"N/A\",\n",
    "                \n",
    "                # Raw values for sorting/plotting later\n",
    "                \"_cv_roc_auc_val\": avg_metrics.get('roc_auc', 0),\n",
    "                \"_test_roc_auc_val\": test_metrics.get('roc_auc', 0) if test_metrics else 0,\n",
    "                \"_cv_recall_val\": avg_metrics.get('recall', 0),\n",
    "                \"_test_recall_val\": test_metrics.get('recall', 0) if test_metrics else 0\n",
    "            }\n",
    "            all_metrics.append(row)\n",
    "            \n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    return df\n",
    "\n",
    "metrics_df = create_metrics_table()\n",
    "# Display columns without the hidden sorting values\n",
    "display_cols = [c for c in metrics_df.columns if not c.startswith(\"_\")]\n",
    "metrics_df[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation Stability (ROC AUC Boxplot)\n",
    "\n",
    "This plot shows the distribution of ROC AUC scores across the 5 folds for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_roc_auc_boxplot():\n",
    "    plot_data = []\n",
    "    \n",
    "    for group, exp_list in EXPERIMENTS.items():\n",
    "        for exp_name in exp_list:\n",
    "            data = load_metrics(exp_name)\n",
    "            if not data:\n",
    "                continue\n",
    "            \n",
    "            fold_metrics = data.get(\"fold_metrics\", [])\n",
    "            for fold in fold_metrics:\n",
    "                plot_data.append({\n",
    "                    \"Experiment\": exp_name,\n",
    "                    \"Group\": group,\n",
    "                    \"ROC AUC\": fold.get(\"roc_auc\", 0)\n",
    "                })\n",
    "                \n",
    "    df_plot = pd.DataFrame(plot_data)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(data=df_plot, x=\"Experiment\", y=\"ROC AUC\", hue=\"Group\", dodge=False)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Cross-Validation ROC AUC Distribution across Folds\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cv_roc_auc_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CV vs Test Performance Comparison\n",
    "\n",
    "We compare the Cross-Validation (mean) performance against the Test Set performance for ROC AUC and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_vs_test_performance(df_metrics):\n",
    "    # Filter out rows without test metrics\n",
    "    df_plot = df_metrics[df_metrics[\"Test ROC AUC\"] != \"N/A\"].copy()\n",
    "    \n",
    "    if df_plot.empty:\n",
    "        print(\"No test metrics available for plotting.\")\n",
    "        return\n",
    "\n",
    "    # Prepare data for plotting (melt)\n",
    "    # ROC AUC\n",
    "    roc_data = []\n",
    "    for _, row in df_plot.iterrows():\n",
    "        roc_data.append({\n",
    "            \"Experiment\": row[\"Experiment\"],\n",
    "            \"Group\": row[\"Group\"],\n",
    "            \"Metric Type\": \"CV\",\n",
    "            \"Score\": row[\"_cv_roc_auc_val\"]\n",
    "        })\n",
    "        roc_data.append({\n",
    "            \"Experiment\": row[\"Experiment\"],\n",
    "            \"Group\": row[\"Group\"],\n",
    "            \"Metric Type\": \"Test\",\n",
    "            \"Score\": row[\"_test_roc_auc_val\"]\n",
    "        })\n",
    "    \n",
    "    df_roc = pd.DataFrame(roc_data)\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.barplot(data=df_roc, x=\"Experiment\", y=\"Score\", hue=\"Metric Type\", palette=\"muted\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylim(0.8, 1.0) # Zoom in as scores are likely high\n",
    "    plt.title(\"ROC AUC: Cross-Validation vs Test Set\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Recall\n",
    "    recall_data = []\n",
    "    for _, row in df_plot.iterrows():\n",
    "        recall_data.append({\n",
    "            \"Experiment\": row[\"Experiment\"],\n",
    "            \"Group\": row[\"Group\"],\n",
    "            \"Metric Type\": \"CV\",\n",
    "            \"Score\": row[\"_cv_recall_val\"]\n",
    "        })\n",
    "        recall_data.append({\n",
    "            \"Experiment\": row[\"Experiment\"],\n",
    "            \"Group\": row[\"Group\"],\n",
    "            \"Metric Type\": \"Test\",\n",
    "            \"Score\": row[\"_test_recall_val\"]\n",
    "        })\n",
    "        \n",
    "    df_recall = pd.DataFrame(recall_data)\n",
    "    \n",
    "    # Plot Recall\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.barplot(data=df_recall, x=\"Experiment\", y=\"Score\", hue=\"Metric Type\", palette=\"pastel\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylim(0.7, 1.0)\n",
    "    plt.title(\"Recall: Cross-Validation vs Test Set\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cv_vs_test_performance(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis\n",
    "\n",
    "We analyze the feature importance for the tree-based models (CatBoost, XGBoost, LightGBM, Random Forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(pipeline, feature_names):\n",
    "    # Try to find the model step\n",
    "    model = None\n",
    "    if hasattr(pipeline, \"named_steps\"):\n",
    "        if \"model\" in pipeline.named_steps:\n",
    "            model = pipeline.named_steps[\"model\"]\n",
    "        elif \"classifier\" in pipeline.named_steps:\n",
    "            model = pipeline.named_steps[\"classifier\"]\n",
    "        else:\n",
    "            # Last step is usually the model\n",
    "            model = pipeline.steps[-1][1]\n",
    "    else:\n",
    "        model = pipeline\n",
    "\n",
    "    # Extract importance\n",
    "    importances = None\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    \n",
    "    if importances is None:\n",
    "        return None\n",
    "        \n",
    "    # Match with feature names\n",
    "    if len(importances) != len(feature_names):\n",
    "        print(f\"Warning: Feature count mismatch. Model: {len(importances)}, Names: {len(feature_names)}\")\n",
    "        return None\n",
    "        \n",
    "    return pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "def plot_feature_importance():\n",
    "    # Only for tree models as requested\n",
    "    target_models = [\n",
    "        \"exp_2_catboost_all\",\n",
    "        \"exp_2_xgboost_all\",\n",
    "        \"exp_2_lgbm_all\",\n",
    "        \"exp_2_random_forest_all\"\n",
    "    ]\n",
    "    \n",
    "    for exp_name in target_models:\n",
    "        print(f\"Processing {exp_name}...\")\n",
    "        exp_dir = BASE_PATH / exp_name\n",
    "        \n",
    "        # Load feature names\n",
    "        feat_file = exp_dir / \"feature_names_all_folds.json\"\n",
    "        if not feat_file.exists():\n",
    "            print(f\"  Feature names file not found for {exp_name}\")\n",
    "            continue\n",
    "            \n",
    "        with open(feat_file, \"r\") as f:\n",
    "            feat_data = json.load(f)\n",
    "            \n",
    "        # Load metrics to find best fold\n",
    "        metrics_data = load_metrics(exp_name)\n",
    "        best_fold = metrics_data.get(\"best_fold\", 1)\n",
    "        \n",
    "        # Get features for best fold\n",
    "        feature_names = next((f[\"features\"] for f in feat_data if f[\"fold\"] == best_fold), None)\n",
    "        if not feature_names:\n",
    "            print(f\"  Could not find feature names for fold {best_fold}\")\n",
    "            continue\n",
    "            \n",
    "        # Load pipeline for best fold\n",
    "        pipeline_path = exp_dir / f\"pipeline_fold_{best_fold}.pkl\"\n",
    "        if not pipeline_path.exists():\n",
    "            print(f\"  Pipeline file not found: {pipeline_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(pipeline_path, \"rb\") as f:\n",
    "                pipeline = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading pipeline: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Get importance\n",
    "        df_imp = get_feature_importance(pipeline, feature_names)\n",
    "        if df_imp is None:\n",
    "            print(f\"  Could not extract feature importance for {exp_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Plot top 20\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=df_imp.head(20), x=\"Importance\", y=\"Feature\", palette=\"viridis\")\n",
    "        plt.title(f\"Top 20 Feature Importance - {exp_name} (Fold {best_fold})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_feature_importance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
